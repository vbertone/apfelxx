%% LyX 2.0.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[twoside,english]{paper}
\usepackage{lmodern}
\renewcommand{\ttdefault}{lmodern}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=3cm,bmargin=2.5cm,lmargin=2cm,rmargin=2cm}
\usepackage{color}
\usepackage{babel}
\usepackage{float}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{esint}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},backref=false,colorlinks=false]
 {hyperref}
\usepackage{breakurl}
\usepackage{makeidx}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}
\numberwithin{figure}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{babel}

\@ifundefined{showcaptionsetup}{}{%
 \PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\makeatother

\usepackage{listings}

\begin{document}

\title{Convolution integrals}

\maketitle

\tableofcontents{}

\vspace{20pt}

This section discusses the approach implemented in {\tt APFEL++} to
compute Mellin-convolution integrals. This kind of convolutions
appears very often in the presence of collinearly factorised
quantities as integrals over the longitudinal momentum fraction of
partons. This is for example the case of all cross sections in
collinear factorisation with hadrons in the initial and/or final
state. As a consequence, Mellin-convolution integrals also appear in
the DGLAP evolution equations. Also in transverse-momentum-dependent
(TMD) factorisation they play an important role when matching TMD
distributions onto the respective collinear ones. It is therefore of
central importance to put in place an efficient and accurate strategy
to numerically compute these convolutions. In doing so, it is also
important to take into account as much as possible all possible
flavours in which they may appear. For example, different variants
apply in the presence of heavy-quark masses or when considering
``non-forward'' integrals in the framework of exclusive physics.

\section{Structure of the integrals}

The general structure of the integrals considered in {\tt APFEL++} has
the form of a Mellin convolution between an \textit{operator} $O$,
that is a potentially complicated known function such as a splitting
function or a partonic cross section,\footnote{More precisely, $O$ is
  in general a distribution that may contain $\delta$ functions and
  $+$-distributions.} and a \textit{distribution} function $d$, as for
example a parton distribution function (PDF) or a fragmentation
function (FF). The explicit form of these integrals reads:
\begin{equation}
I(x) = x\int_0^1dz\int_0^1dy\, O(z)d(y)\delta(x-yz) =
x\int_x^1\frac{dz}{z} O \left(\frac{x}{z}\right) d(z) =
x\int_x^1\frac{dz}{z} O(z)d\left(\frac{x}{z}\right)\,.
\label{eq:ZMconv}
\end{equation}
Typically in the presence of mass effects, the integration phase space
may be modified and the convolution in Eq.~(\ref{eq:ZMconv}) is
generalised as:
\begin{equation}
I(x,\eta) = x\int_{x/\eta}^1\frac{dz}{z} O(z,\eta)d\left(\frac{x}{\eta
    z}\right)\,,
\label{eq:Genconv}
\end{equation}
where $\eta\leq1$, with $\eta = 1$ reproducing
Eq.~(\ref{eq:ZMconv}). However, it is convenient to rewrite the
integral in Eq.~(\ref{eq:Genconv}) in a form in which the lower bound
of the integral is equal to $x$. This is easily done by performing the
change of variable $y =\eta z$, so that:
\begin{equation}
  I(x,\eta)= \int_x^\eta dy\,
  O\left(\frac{y}{\eta},\eta\right)\,\frac{x}{y}d\left(\frac{x}{y}\right)\,.
\label{eq:Genconv1}
\end{equation}

In order to enable the precomputation the expensive part of the
integral in Eq.~(\ref{eq:Genconv1}), we use the standard interpolation
formula on the distribution $d$:
\begin{equation}
\frac{x}{y} d\left(\frac{x}{y}\right) = \sum_{\alpha=0}^{N_x} x_\alpha
d(x_\alpha) w_{\alpha}^{(k)}\left(\frac{x}{y}\right)\,,
\label{eq:Interpolation}
\end{equation}
where $\alpha$ runs over the nodes of a grid in $x$ and the weights
$w_{\alpha}$ are assumed to be Langrange polynomials of degree
$k$. Now we plug Eq.~(\ref{eq:Interpolation}) into
Eq.~(\ref{eq:Genconv1}) and specialise the computation of the integral
$I$ to the girid point $x_\beta$. This gives:
\begin{equation}
  I(x_\beta,\eta)=  \sum_{\alpha=0}^{N_x} \overline{d}_\alpha\int_{x_\beta}^\eta dy\,
  O\left(\frac{y}{\eta},\eta\right)\,w_{\alpha}^{(k)}\left(\frac{x_\beta}{y}\right)\,.
\end{equation}
where we have defined $\overline{d}_\alpha = x_\alpha d(x_\alpha)$.
As shown in the section devoted to the interpolation procedure, under
some specific conditions the interpolating functions are such that:
\begin{equation}
w_{\alpha}^{(k)}\left(\frac{x_\beta}{y}\right)\neq 0\quad\mbox{for}\quad c < y < d\,,
\end{equation}
with:
\begin{equation}
  c =
  \mbox{max}(x_\beta,x_\beta/x_{\alpha+1}) \quad\mbox{and}\quad d =
  \mbox{min}(\eta,x_\beta/x_{\alpha-k}) \,.
\label{eq:intlims}
\end{equation}
Therefore, Eq.~(\ref{eq:Genconv1}) can be adjusted as:
\begin{equation}
  I(x_\beta,\eta)=  \sum_{\alpha=0}^{N_x} \overline{d}_\alpha\int_c^d dy\,
  O\left(\frac{y}{\eta},\eta\right)\,w_{\alpha}^{(k)}\left(\frac{x_\beta}{y}\right)\,.
\label{eq:Genconv2}
\end{equation}
Finally, we change the integration variable back to $z=y/\eta$ so that
Eq.~(\ref{eq:Genconv2}) becomes:
\begin{equation}
  I(x_\beta,\eta)=  \sum_{\alpha=0}^{N_x} \overline{d}_\alpha \underbrace{\left[\eta\int_{c/\eta}^{d/\eta} dz\,
  O\left(z,\eta\right)\,w_{\alpha}^{(k)}\left(\frac{x_\beta}{\eta
      z}\right)\right]}_{\Gamma_{\beta \alpha}}=\sum_{\alpha=0}^{N_x} \Gamma_{\beta \alpha}\overline{d}_\alpha\,.
\label{eq:Genconv3}
\end{equation}
To summarise, Eq.~(\ref{eq:Genconv3}) allows one to compute the
integral $I$ on the nodes of a given grid as a weighted sum of the
values of the distribution $d$ on the nodes themselves. The weights
are given by the appropriate integral of the operator $O$ and the
interpolating functions $w_{\alpha}^{(k)}$. The value of $I$ for a
generic value of $x$ can finally be obtained by interpolation using a
formula similar to Eq.~(\ref{eq:Interpolation}). The interpolation can
be manipulated as follows:
\begin{equation}
I(x,\eta) = \sum_{\beta=0}^{N_x} w_\beta^{(k)}(x) I(x_\beta,\eta) = \sum_{\alpha=0}^{N_x}\underbrace{\left[\sum_{\alpha=0}^{N_x} w_\beta^{(k)}(x) \Gamma_{\beta \alpha}\right]}_{g_\alpha(x)}\overline{d}_\alpha=\sum_{\alpha=0}^{N_x}g_\alpha(x)\overline{d}_\alpha\,.
\end{equation}
Therefore, the operator $\Gamma_{\beta\alpha}$ itself can be
interpolated in $x$ over the first index $\beta$ thus obtaining the
distribution $g_\alpha(x)$ that can ``contracted'' with the
distribution $\overline{d}_\alpha$ to obtain the integral $I$ in
$x$. In conclusion, Eq.~(\ref{eq:Genconv3}) provides a fast way of
computing convolution integrals. Indeed, the weights
$\Gamma_{\beta\alpha}$ can be precomputed and stored once and for all
and the integral $I$ can be quickly computed with different
distributions $d$ by simply taking a weighted sum.

We now need an operational way to compute the weights
$\Gamma_{\beta\alpha}$. To do so, it is necessary to know the general
structure of the operator $O$. Quite generally, the operator $O$
splits as follows:
\begin{equation}
  O(z,\eta) = R(z,\eta)+\sum_{i}\left[P^{(i)}(z,\eta)\right]_+
  S^{(i)}(z,\eta)+ L(\eta)\delta(1-z)\,,
\label{eq:CoeffFuncs}
\end{equation}
where $R$ and $S^{(i)}$ are a regular functions at $z=1$, that is:
\begin{equation}
R(1,\eta) =  \lim_{z\rightarrow 1} R(z,\eta) = K(\eta)\quad\mbox{and}\quad S^{(i)}(1,\eta) =  \lim_{z\rightarrow 1} S^{(i)}(z,\eta) = J^{(i)}(\eta)\,,
\end{equation}
being $K$, $J^{(i)}$, and $L$ finite functions of $\eta$. The
functions $P^{(i)}$ are instead singular at $z=1$ and non-integrable
in the limit $\eta\rightarrow1$ and thus are regularised through the
$+$-prescription. Plugging Eq.~(\ref{eq:CoeffFuncs}) into the
definition of $\Gamma_{\beta\alpha}$ in Eq.~(\ref{eq:Genconv3}) and
making use of the definition of $+$-prescription gives:
\begin{equation}
\begin{array}{rcl}
\Gamma_{\beta\alpha} &=& \eta\displaystyle \int_{c/\eta}^{d/\eta} dz\left\{R\left(z,\eta\right)w_{\alpha}^{(k)}\left(\frac{x_\beta}{\eta z}\right)+\sum_{i}P^{(i)}(z,\eta)\left[S^{(i)}\left(z,\eta\right)w_{\alpha}^{(k)}\left(\frac{x_\beta}{\eta z}\right)-S^{(i)}(1,\eta)w_{\alpha}^{(k)}\left(\frac{x_\beta}{\eta}\right)\right]\right\}\\
\\
&+&\displaystyle \eta\left[L(\eta)-\sum_{i}S^{(i)}(1,\eta)\int_0^{c/\eta}dz
    P^{(i)}(z,\eta)\right]w_{\alpha}^{(k)}\left(\frac{x_\beta}{\eta}\right)\,,
\end{array}
\label{eq:FinalExpression}
\end{equation}
that can be further manipulated changing the integration variable into
$y=\eta z$:
\begin{equation}
\begin{array}{rcl}
  \Gamma_{\beta\alpha} &=& \displaystyle \int_{c}^{d} dy\left\{R\left(\frac{y}{\eta},\eta\right)w_{\alpha}^{(k)}\left(\frac{x_\beta}{y}\right)+\sum_{i}P^{(i)}\left(\frac{y}{\eta},\eta\right)\left[S^{(i)}\left(\frac{y}{\eta},\eta\right)w_{\alpha}^{(k)}\left(\frac{x_\beta}{y}\right)-S^{(i)}(1,\eta)w_{\alpha}^{(k)}\left(\frac{x_\beta}{\eta}\right)\right]\right\}\\
  \\
                       &+&\displaystyle \eta \left[L(\eta)+\sum_{i}S^{(i)}(1,\eta)Q^{(i)}\left(\frac{c}{\eta},\eta\right)\right]w_{\alpha}^{(k)}\left(\frac{x_\beta}{\eta}\right)\,.
\end{array}
\label{eq:FinalExpression2}
\end{equation}
where we have defined:
\begin{equation}
  Q^{(i)}(a,\eta)\equiv-\int_0^{a}dz P^{(i)}\left(z,\eta\right).
\end{equation}
These integrals can, most of the times, be computed analytically.

Eqs.~(\ref{eq:FinalExpression})-(\ref{eq:FinalExpression2}) expose the
full potential complexity of the task of computing the weights
$\Gamma_{\beta\alpha}$. However, in many practical applications some
simplifications apply. For example, in the case of the perturbative
coefficients of the ($\overline{\mbox{MS}}$) splitting functions there
are two simplications: the first is that $\eta=1$, the second is that
there is one single term in the sum over $i$ ($i=0$) such that and the
general form of the function $P$ is:
\begin{equation}
P^{(0)}(z,\eta)\rightarrow \frac1{1-z}\,,
\end{equation}
so that:
\begin{equation}
Q^{(0)}(a,\eta)=\ln(1-a)\,.
\end{equation}
Considering that:
\begin{equation}
w_\alpha^{(k)}(x_\beta) = \delta_{\alpha\beta}\,,
\end{equation}
and that the expressions can always be manipulated in such a way that
the coefficient of the $+$-prescripted term $S$ is a constant, one
finds that:
\begin{equation}
\Gamma_{\beta\alpha} = \int_{c}^{d}
dz\left\{R\left(z\right)w_{\alpha}^{(k)}\left(\frac{x_\beta}{z}\right)+\frac{S}{1-z}\left[w_{\alpha}^{(k)}\left(\frac{x_\beta}{z}\right)-\delta_{\alpha\beta}\right]\right\}+\displaystyle
\left[S\ln\left(1-c\right)+L \right]\delta_{\alpha\beta}\,.
\label{eq:SplittingFunctions}
\end{equation}

The same kind of simplifications applies to the case of the
perturbative Zero-Mass (ZM) coefficient functions with the only
exception that the sum over $i$ extends to more terms depending on the
perturbative order. For example at $\mathcal{O}(\alpha_s)$, $i.e.$ at
NLO, the general form of the coefficient functions reads:
\begin{equation}
\begin{array}{rcl}
  \Gamma_{\beta\alpha} &=&\displaystyle \int_{c}^{d}
                           dz\left\{R\left(z\right)w_{\alpha}^{(k)}\left(\frac{x_\beta}{z}\right)
  + \frac{S^{(0)}+S^{(1)}\ln(1-z)}{1-z}\left[w_{\alpha}^{(k)}\left(\frac{x_\beta}{z}\right)-\delta_{\alpha\beta}\right]\right\}\\
\\
&+&\displaystyle
    \left[S^{(0)}\ln\left(1-c\right)+\frac12
    S^{(1)}\ln^2\left(1-c\right)+L \right]\delta_{\alpha\beta}\,.
\end{array}
\label{eq:ZMCoeffFunctions}
\end{equation}

Things get more complicated when considering massive coefficient
functions. In this case, the role of $\eta$ is played by the ratio
$m/Q$, where $m$ is the mass of the quark and $Q$ the hard scale. In
addition, the $+$-prescripted contributions can become more
convoluted.

As an aside, it is interesting to notice that we have silently decided
to use one of the two possible ways to compute the convolution
integral $I(x)$ in Eq.~(\ref{eq:ZMconv}). Specifically, we have chosen
the rightmost equality. However, the equality:
\begin{equation}
I(x) = x\int_x^1\frac{dz}{z} O \left(\frac{x}{z}\right) d(z)\,,
\end{equation}
is equally valid. If one choses this particular version of the
convolution integral and follows the interpolation procedure discussed
above, the result for $I$ computed in $x_\beta$ would be identical to
Eq.~(\ref{eq:Genconv3}) (with $\eta=1$) except for the fact that the
weights $\Gamma_{\beta \alpha}$ are computed as:
\begin{equation}
\Gamma_{\beta\alpha} = \int_{a}^{b}dz\,
O\left(\frac{x_\beta}{z}\right)w_{\alpha}^{(k)}(z)
\label{eq:oldway}
\end{equation}
with:
\begin{equation}
a \equiv \mbox{max}(x_\beta,x_{\alpha-k})\quad\mbox{and}\quad b \equiv \mbox{min}(1,x_{\alpha+1})\,.
\end{equation}
This immediately implies that the integral in Eq.~(\ref{eq:Genconv3})
and that in Eq.~(\ref{eq:oldway}) have to coincide. This is indeed the
case and the only reason to choose Eq.~(\ref{eq:Genconv3}) rather than
Eq.~(\ref{eq:oldway}) is convenience. Specifically,
Eq.~(\ref{eq:oldway}) implies computing the operator $O$ at
$x_\beta/z$ rather than at $z$ and this makes the treatment of
$\delta$-functions and $+$-distributions possibly present in $O$ a
little more delicate. However, Eq.~(\ref{eq:oldway}) remains a valid
alternative that we just decided not to pursue.

\subsection{Double convolutions}

The technique discussed above to compute convolution integrals as
weighted sums of a given input distributions $d$ can be, under certain
circumstances, generalised to convolution integrals involving
\textit{two} distributions. Relevant examples are the Drell-Yan (DY)
and the semi-inclusive deep-inelastic scattering (SIDIS) cross
sections integrated over the transverse momentum of the virtual
boson. In these cases the cross section is proportional to double
convolutions between partonic cross sections and a pair of
non-perturbative distributions. The general form of this double
convolution is:
\begin{equation}
J(x_1,x_2) =
x_1 x_2\int_{x_1}^1\frac{dy_1}{y_1}\int_{x_2}^1\frac{dy_2}{y_2}
O(y_1,y_2) d^{(1)}\left(\frac{x_1}{y_1}\right)
  d^{(2)}\left(\frac{x_2}{y_2}\right)\,.
\end{equation}
Applying the same interpolation procedure as in the single-convolution
case gives:
\begin{equation}
\begin{array}{rcl}
J(x_\delta,x_\gamma) &=&\displaystyle
\int_{x_\delta}^1dy_1\int_{x_\gamma}^1dy_2\,
O(y_1,y_2) \left[\frac{x_\delta}{y_1}d^{(1)}\left(\frac{x_\delta}{y_1}\right)\right]
  \left[\frac{x_\gamma}{y_2}d^{(2)}\left(\frac{x_\gamma}{y_2}\right)\right]\\
\\
&=&\displaystyle
    \sum_{\alpha=0}^{N_x}\sum_{\beta=0}^{N_x} \overline{d}^{(1)}_\beta
    \overline{d}^{(2)}_\alpha \underbrace{\left[\int_{x_\delta}^1 dy_1 \int_{x_\gamma}^1 dy_2\,
    O(y_1,y_2)\, w_{\beta}^{(k)}\left(\frac{x_\delta}{y_1}\right) 
    w_{\alpha}^{(k)}\left(\frac{x_\gamma}{y_2}\right)\right]}_{\Theta^{\beta\delta,\alpha\gamma}}\,.
\end{array}
\label{eq:DoubleZMconv}
\end{equation}
In Eq.~(\ref{eq:DoubleZMconv}) we have assumed that there are no mass
corrections and thus the convolutions take the simplest form. In the
case of double convolutions, the operator $O$ is a function of two
variables, $y_1$ and $y_2$, and, as in the case of the single
convolutions, it receives three kinds of contribution in both these
variables: local terms proportional to $\delta$-functions, singular
terms proportional to $+$-prescripted functions, and regular
terms. The complication here is that these contributions from the two
variables $y_1$ and $y_2$ mix and thus, for example, terms local in
$y_1$ and singular in $y_2$ may also appear. It is thus necessary to
identify the general structure of the function $O$ to see whether it
is possible to decompose the double operator
$\Theta^{\beta\delta,\alpha\gamma}$ into products of single operators.

As we will explicitly show below, in the case of SIDIS up to
$\mathcal{O}(\alpha_s)$ (NLO) the general structure of the function
$O$ can be inferred looking at Eqs.~(C.2)-(C.7) of
Ref.~\cite{deFlorian:1997zj}:
\begin{equation}
\begin{array}{rclclcl}
  O(y_1,y_2) &=&\displaystyle {\rm LL}\,\delta(1-y_1)\delta(1-y_2) &+&\displaystyle {\rm LS}\,
                                                                       \delta(1-y_1)\left[\frac{\ln(1-y_2)}{1-y_2}\right]_+ &+&\displaystyle \delta(1-y_1)\,{\rm LR}(y_2) \\
  \\
             &+&\displaystyle {\rm SL}\,\left[\frac{\ln(1-y_1)}{1-y_1}\right]_+\delta(1-y_2)
                                                                   &+&\displaystyle {\rm SS}\,\left[\frac1{1-y_1}\right]_+
                                                                       \left[\frac1{1-y_2}\right]_+ &+&\displaystyle \left[\frac1{1-y_1}\right]_+\,{\rm
                                                                                                        SR}(y_2)\\
  \\
             &+&\displaystyle {\rm RL}(y_1)\,\delta(1-y_2)   &+&\displaystyle
                                                                 {\rm RS}(y_1)
                                                                 \,\left[\frac1{1-y_2}\right]_+&+&\displaystyle
                                                                                                   \sum_iK_i{\rm
                                                                                                   R}_i^{(1)}(y_1){\rm
                                                                                                   R}_i^{(2)}(y_2)\,.
\end{array}
\label{eq:DoubleFuncStruct}
\end{equation}
It is clear that in Eq.~(\ref{eq:DoubleFuncStruct}) all terms
factorise into a part that only depends on $y_1$ and a part that only
depends on $y_2$. This is the crucial feature that enables use the
technology developed above for the single convolutions. Plugging
Eq.~(\ref{eq:DoubleFuncStruct}) into Eq.~(\ref{eq:DoubleZMconv}), one
finds that:
\begin{equation}
\begin{array}{rclclcl}
  \Theta^{\beta\delta,\alpha\gamma} &=& {\rm LL} \,
                                        \Gamma^{\rm L}_{\beta\delta}\Gamma^{\rm L}_{\alpha\gamma}
  &+& \displaystyle {\rm LS}\,\Gamma^{\rm L}_{\beta\delta}\Gamma_{\alpha\gamma}^{\rm S1}
  &+&\displaystyle \Gamma^{\rm L}_{\beta\delta} \Gamma_{\alpha\gamma}^{\rm LR}\\
  \\
                                    &+&{\rm SL}\,\displaystyle\Gamma_{\beta\delta}^{\rm S1}\Gamma^{\rm L}_{\alpha\gamma} 
  &+&
     {\rm SS}\,\Gamma_{\beta\delta}^{\rm
      S0}\Gamma_{\alpha\gamma}^{\rm S0}&+& \Gamma_{\beta\delta}^{\rm S0}\Gamma_{\alpha\gamma}^{\rm SR}\\
\\
&+& \displaystyle \Gamma_{\beta\delta}^{\rm RL}\Gamma^{\rm L}_{\alpha\gamma} &+& \Gamma_{\beta\delta}^{\rm RS}\Gamma_{\alpha\gamma}^{\rm
      S0} &+& \displaystyle \sum_i K_i \Gamma_{\beta\delta}^{R_i^{(1)}}\Gamma_{\alpha\gamma}^{R_i^{(2)}}
\end{array}
\label{eq:MasterFormula}
\end{equation}
with:
\begin{equation}
\begin{array}{rcl}
  \Gamma_{\alpha\beta}^{\rm L} &=& \displaystyle  \int_{c_{\alpha\beta}}^{d_{\alpha\beta}}
                                       dz\,\delta(1-z)w_{\beta}^{(k)}\left(\frac{x_\alpha}{z}\right)
                                   =
                                   w_{\beta}^{(k)}\left(x_\alpha\right)
                                   =  \delta_{\alpha\beta}\\
\\
  \Gamma_{\alpha\beta}^{{\rm S}n} &=& \displaystyle  \int_{c_{\alpha\beta}}^{d_{\alpha\beta}}
                                       dz\frac{\ln^n(1-z)}{1-z}\left[w_{\beta}^{(k)}\left(\frac{x_\alpha}{z}\right)-\delta_{\alpha\beta}\right]+\frac1{n+1}\ln^{n+1}\left(1-c_{\alpha\beta}\right)\delta_{\alpha\beta}\\
  \\
  \Gamma_{\alpha\beta}^{f} &=&\displaystyle \int_{c_{\alpha\beta}}^{d_{\alpha\beta}}
                                     dz\,f(z)\,w_{\beta}^{(k)}\left(\frac{x_\alpha}{z}\right)
\end{array}
\label{eq:PrecompOp}
\end{equation}
where $f$ is a regular function and the integration bounds are defined
as:
\begin{equation}
  c_{\alpha\beta} =
  \mbox{max}(x_\alpha,x_\alpha/x_{\beta+1}) \quad\mbox{and}\quad d_{\alpha\beta} =
  \mbox{min}(1,x_\alpha/x_{\beta-k}) \,.
\end{equation}
Notice that $c_{\alpha\beta}$ appears in the term proportional to
$\delta_{\alpha\beta}$, so that one has:
\begin{equation}
  c_{\alpha\alpha} = x_\alpha/x_{\alpha+1}\,,
\end{equation}
so that, if the grid is logarithmically spaced, $c_{\alpha\alpha}$ is
a constant, \textit{i.e.} it does not depend on the grid index
$\alpha$. In general, we assume that it is always possible to write an
object of the kind of $\Theta^{\beta\delta,\alpha\gamma}$ as series of
bilinear terms:
\begin{equation}
\Theta^{\beta\delta,\alpha\gamma} = \sum_j
C_j\Gamma_j^{(1),\beta\delta}\Gamma_j^{(2),\alpha\gamma}
\label{eq:OpSeries}
\end{equation}
where $C_j$ are scalar coefficients, and the weights
$\Gamma_j^{(1),\beta\delta}$ and $\Gamma_j^{(2),\alpha\gamma}$ can be
computed using the technology discussed in the previous
section. Plugging Eq.~(\ref{eq:OpSeries}) into
Eq.~(\ref{eq:DoubleZMconv}), one finds that:
\begin{equation}
  J(x_\delta,x_\gamma) =
  \sum_j C_j  f_j^{(1),\delta} f_j^{(2),\gamma}\,,
\label{eq:DoubleZMconv2}
\end{equation}
where we have defined:
\begin{equation}
f_j^{(1),\delta}\equiv \sum_{\beta=0}^{N_x} \overline{d}^{(1)}_\beta\Gamma_j^{(1),\beta\delta}\quad\mbox{and}\quad f_j^{(2),\gamma}\equiv \sum_{\alpha=0}^{N_x}\overline{d}^{(2)}_\alpha \Gamma_j^{(2),\alpha\gamma}\,.
\end{equation}

Eq.~(\ref{eq:DoubleZMconv2}) shows that, under the hypothesis that the
operator $O(y_1,y_2)$ can be expressed as a series of terms whose
dependence on $y_1$ and $y_2$ factorizes,\footnote{This is the case
  for SIDIS and DY up to NLO. However, one may expect that this
  feature holds also beyond.} the double convolution in
Eq.~(\ref{eq:DoubleZMconv}) is given by a series of bilinear
distributions ($f_j^{(1),\delta}$ and $f_j^{(2),\gamma}$) singularly
obtained as convolutions of single operators with the input
distributions $d^{(1)}$ and $d^{(2)}$. This is a particularly useful
achievement that allows us to compute double convolutions without the
need of extending the integration and the interpolation procedures to
two dimensions with an obvious gain in accuracy and performance. As a
matter of fact, the same argument can be extended to a multiple
convolution of the function $O(\{y_i\})$, which again can be expressed
as a series of $n$-linear terms, with $i=1,\dots,n$, with $n$
distributions:
\begin{equation}
J(\{x_{\alpha_i}\}) = \sum_j C_j  \prod_{i=1}^nf_j^{(i),\alpha_i}\,,
\end{equation}
with:
\begin{equation}
f_j^{(i),\alpha_i} \equiv \sum_{\beta=0}^{N_x} \overline{d}^{(i)}_\beta\Gamma_j^{(i),\beta\alpha_i}\,.
\end{equation}
This technology could be useful for more complicated observables, like
cross sections in $pp$ collisions with an identified hadron in the
final state, that requires for example three convolutions.

The challenging part of the procedure just presented resides in the
``pre-processing'' of the function $O(y_1,y_2)$ that has to be
analytically manipulated to disentangle the single terms. This step,
however, has to be taken only once.

Before going into a concrete application, it is useful to connect
Eq.~(\ref{eq:MasterFormula}) to Eq.~(\ref{eq:OpSeries}) by identifying
number and form of the coefficients and weights
involved. Specifically, assuming that the series in the last term in
the r.h.s. of Eq.~(\ref{eq:MasterFormula}) has $r$ terms, the series
in Eq.~(\ref{eq:OpSeries}) will have $8+r$ terms, that is:
\begin{equation}
\Theta^{\beta\delta,\alpha\gamma} = \sum_{j=1}^{8+r}
C_j\Gamma_j^{(1),\beta\delta}\Gamma_j^{(2),\alpha\gamma}
\label{eq:OpSeriesCon}
\end{equation}
with:
\begin{equation}
\begin{array}{lclll}
j = 1 &:& C_1 = {\rm LL}, & \Gamma_1^{(1),\beta\delta} = \Gamma^{\rm L}_{\beta\delta}, & \Gamma_1^{(2),\alpha\gamma} = \Gamma^{\rm L}_{\alpha\gamma},\\
j = 2 &:& C_2 = {\rm LS}, & \Gamma_2^{(1),\beta\delta} = \Gamma^{\rm L}_{\beta\delta}, & \Gamma_2^{(2),\alpha\gamma} = \Gamma_{\alpha\gamma}^{\rm S1},\\
j = 3 &:& C_3 = 1, & \Gamma_3^{(1),\beta\delta} = \Gamma^{\rm L}_{\beta\delta}, & \Gamma_3^{(2),\alpha\gamma} = \Gamma_{\alpha\gamma}^{\rm LR},\\
j = 4 &:& C_4 = {\rm SL}, & \Gamma_4^{(1),\beta\delta} = \Gamma_{\beta\delta}^{\rm S1}, & \Gamma_4^{(2),\alpha\gamma} = \Gamma^{\rm L}_{\alpha\gamma},\\
j = 5 &:& C_5 = {\rm SS},& \Gamma_5^{(1),\beta\delta} = \Gamma_{\beta\delta}^{\rm S0}, & \Gamma_5^{(2),\alpha\gamma} = \Gamma_{\alpha\gamma}^{\rm S0},\\
j = 6 &:& C_6 = 1, & \Gamma_6^{(1),\beta\delta} = \Gamma_{\beta\delta}^{\rm S0}, & \Gamma_6^{(2),\alpha\gamma} = \Gamma_{\alpha\gamma}^{\rm SR},\\
j = 7 &:& C_7 = 1, & \Gamma_7^{(1),\beta\delta} = \Gamma_{\beta\delta}^{\rm RL}, & \Gamma_7^{(2),\alpha\gamma} = \Gamma^{\rm L}_{\alpha\gamma},\\
j = 8 &:& C_8 = 1, & \Gamma_8^{(1),\beta\delta} = \Gamma_{\beta\delta}^{\rm RS}, & \Gamma_8^{(2),\alpha\gamma} = \Gamma_{\alpha\gamma}^{\rm S0},\\
j = 9 &:& C_9 = K_1 & \Gamma_9^{(1),\beta\delta} = \Gamma_{\beta\delta}^{R_1^{(1)}}, & \Gamma_9^{(2),\alpha\gamma} = \Gamma_{\alpha\gamma}^{R_1^{(2)}},\\
\vdots & & & & \\
j = 8 + r &:& C_{8+r} = K_r & \Gamma_{8+r}^{(1),\beta\delta} = \Gamma_{\beta\delta}^{R_r^{(1)}}, & \Gamma_{8+r}^{(2),\alpha\gamma} = \Gamma_{\alpha\gamma}^{R_r^{(2)}}
\end{array}
\end{equation}
It should be noted that, despite the large number of terms in the
series in Eq.~(\ref{eq:OpSeriesCon}), the number of weights to be
precomputed is usually pretty limited. In addition, in many cases many
of the terms of the series are zero so that the number of
contributions is further reduced. We can now apply this procedure up
to NLO in QCD to two specific cases: SIDIS first and DY second
(incomplete).

\subsubsection{Semi-inclusive deep inelastic scattering (SIDIS)}

the structure of the ($p_T$-integrated) SIDIS cross section and the
expressions of the respective hard coefficient functions can be found
in Ref.~\cite{deFlorian:1997zj}. Following this paper, the SIDIS
differential cross section for the exchange of a virtual photon can be
written as:
\begin{equation}
  \frac{d^3\sigma}{dxdydz} = 
  \frac{2\, \pi\alpha^2}{xyQ^2} 
  \left[ (1+(1-y)^2) 2xF_1(x,z,Q^2) + 
    2 (1-y) F_L(x,z,Q^2) \right]\,,
\label{eq:sidis}
\end{equation}
with $Q^2 = - q^2$ the (negative) virtuality of the exchanged photon,
$x$ and $z$ the momentum fractions that enter the PDFs and the FFs,
and $y = Q^2/xs$ the inelasticity given in terms of $Q$, $x$, and the
collision energy in the center of mass $s$. Notice that, as compared
to Ref.~\cite{deFlorian:1997zj}, we have absorbed a factor $x$ into
the definition of $F_L$ as customary for the longitudinal structure
function in inclusive DIS.

We now use the Callan-Gross relation:
\begin{equation}
F_2 = 2xF_1 + F_L
\label{eq:CallanGross}
\end{equation}
to replace $2xF_1$ with $F_2$ in Eq.~(\ref{eq:sidis}):
\begin{equation}
  \frac{d^3\sigma}{dxdydz} = 
  \frac{2\, \pi\alpha^2}{xyQ^2} 
  \left[ Y_+ F_2(x,z,Q^2)
    -y^2 F_L(x,z,Q^2) \right]\,,
\label{eq:sidis2}
\end{equation}
where we have defined:
\begin{equation}
Y_+ = 1+(1-y)^2\,.
\end{equation}
It is also useful to write Eq.~(\ref{eq:sidis2}) as differential in
$x$, $Q^2$, and $z$:
\begin{equation}
  \frac{d^3\sigma}{dx\, dQ^2\, dz} = 
  \frac{2\, \pi\alpha^2}{xQ^4} 
  \left[ Y_+ F_2(x,z,Q)
    -y^2 F_L(x,z,Q) \right]\,.
\label{eq:sidis3}
\end{equation}
The structure functions $F_2$ and $F_L$ are given at NLO by:
\begin{equation}
\begin{array}{rcl}
\displaystyle  F_{2,L}(x,z,Q) &=& \displaystyle x\sum_{q,\overline{q}} e_q^2 \bigg[ q(x,Q)
    \otimes  C^{2,L}_{qq}(x,z) \otimes D_q(z,Q)  \\
&+&\displaystyle q(x,Q)  \otimes  C^{2,L}_{gq}(x,z) \otimes D_g(z,Q)+  g(x,Q)  \otimes
  C^{2,L}_{qg}(x,z) \otimes D_q(z,Q) \bigg]\,,
\end{array}
\label{eq:f1sidis}
\end {equation}
where $\{q,g\}$ are the quark and gluon PDFs and $\{D_q,D_g\}$ are the
quark and gluon FFs, $e_q$ is the electric charge of the quark $q$ and
$\{C^{2,L}_{qq},C^{2,L}_{qg},C^{2,L}_{gq}\}$ are the relevant partonic
cross sections. The partonic cross sections admit a perturbative
expansion in power of $\alpha_s$:
\begin{equation}
C = \sum_{n=0} \left(\frac{\alpha_s}{4\pi}\right)^nC^{(n)}
\end{equation}
that we truncate to NLO, $i.e.$ to $n=1$. At LO ($n=0$) we have the simple
expression:
\begin{equation}
\begin{array}{rcl}
C^{2,(0)}_{qq}(x,z) &=& \delta(1-x)\delta(1-z)\,,\\
\\
C^{2,(0)}_{qg}(x,z) &=&C^{2,(0)}_{gq}(x,z) = 0\,.
\end{array}
\end{equation}

At NLO ($n=1$) we take the expressions from Appendix C of
Ref.~\cite{deFlorian:1997zj} being careful to take into account an
additional factor two due to the difference in the expansion parameter
($\alpha_s/4\pi$ rather than $\alpha_s/2\pi$). We also need to combine
the expressions for $F_1$ and $F_L$ using Eq.~(\ref{eq:CallanGross})
to obtain the partonic cross sections for $F_2$. We start with the
partonic cross sections for $F_L$ that read:
\begin{equation}
\begin{array}{rcl}
C_{qq}^{L,(1)} &=& 8 C_F x z\,, \\
C_{gq}^{L,(1)} &=& 8 C_F x (1-z)\,, \\
C_{qg}^{L,(1)} &=& 8 x(1-x)\,, 
\end{array}
\label{eq:cfFL}
\end{equation}
while those for $F_2$ read:
\begin{equation}
\begin{array}{rcl}
  \displaystyle \frac{C_{qq}^{2,(1)}}{2C_F} &=& \displaystyle -8\delta(1-x)\delta(1-z)+2\delta(1-x)
                                  \left(\frac{\ln
                                  (1-z)}{1-z}\right)_++ 
                                  \delta(1-x) \left[\frac{1+z^2}{1-z}\ln z+(1-z)-(1+z)\ln(1-z)\right] \\
  \\
                              &+&\displaystyle 
                                  2\left(\frac{\ln
                                  (1-x)}{1-x}\right)_+\delta(1-z)+2\left(\frac{1}{1-x}\right)_+\left(\frac{1}{1-z}\right)_+- \left(\frac{1}{1-x}\right)_+(1+z)\\
  \\
                              &+& \displaystyle \left[ 
                                  -\frac{1+x^2}{1-x}\ln x+(1-x)
                                  -(1+x)\ln(1-x)\right]\delta(1-z) -
                                  (1+x)\left(\frac{1}{1-z}\right)_++(2+6xz)\,,\\
  \\
  \displaystyle \frac{C_{gq}^{2,(1)}}{2C_F} &=& \displaystyle \delta (1-x) \left[\frac{1+(1-z)^2}{z} \ln\left[
                                               z(1-z)\right]+z\right]\\
  \\
                                            &+&\displaystyle \left(\frac{1}{1-x}\right)_+\frac{1+(1-z)^2}{z}\\
  \\
                              &+&  \displaystyle 2(1+3x)-6xz-(1+x)\frac{1}{z}\,,\\
  \\
  C_{qg}^{2,(1)} &=& \displaystyle \left[(x^2+(1-x)^2)
                     \ln\left(\frac{1-x}{x}\right)
                     +2x(1-x)\right]\delta (1-z) +(x^2+(1-x)^2)
                     \left(\frac{1}{1-z}\right)_+\\
\\
&+&\displaystyle 2(-1+6x-6x^2) + (x^2+(1-x)^2) \frac{1}{z} \,.
\end{array}
\label{eq:cfF2}
\end{equation}
By inspection of Eqs.~(\ref{eq:cfFL}) and~(\ref{eq:cfF2}) we can
deduce the various coefficients of
Eq.~(\ref{eq:DoubleFuncStruct}). $F_L$ involves only regular functions
so that all contributions are zero but the fully regular ones:
\begin{equation}
\begin{array}{rcl}
C_{qq}^{L,(1)}(x,z) &:& K_1 = 8 C_F\,,\quad R_1^{(1)}(x) = x\,,\quad
                        R_1^{(2)}(z) = z\,,\\
C_{gq}^{L,(1)}(x,z) &:& K_1 = 8 C_F\,,\quad R_1^{(1)}(x) = x\,,\quad
                        R_1^{(2)}(z) = 1-z\,,\\
C_{qg}^{L,(1)}(x,z) &:& K_1 = 8\,,\quad R_1^{(1)}(x) = x(1-x)\,,\quad
                        R_1^{(2)}(z) = 1\,.\\
\end{array}
\end{equation}

The situation is more complicated for $F_2$ but we can still identify
the different contributions:
\begin{equation}
\begin{array}{rcl}
C_{qq}^{2,(1)} &:& \displaystyle {\rm LL} = -16 C_F\,,\quad {\rm LS} =
                        4C_F\,,\quad {\rm LR}(z) = 2C_F
                        \left[\frac{1+z^2}{1-z}\ln
                        z+(1-z)-(1+z)\ln(1-z)\right]\\
\\
&& \displaystyle {\rm SL} = 4C_F\,,\quad {\rm SS} = 4C_F\,,\quad {\rm
   SR}(z) = -2C_F(1+z)\\
\\
&& \displaystyle {\rm RL}(x) = 2C_F \left[ 
                                  -\frac{1+x^2}{1-x}\ln x+(1-x)
                                  -(1+x)\ln(1-x)\right]\,,\quad {\rm
   RS}(x) = -2C_F(1+x)\,,\\
\\
&& \displaystyle \left\{K_1 =  4C_F,\, R_1^{(1)}(x) = 1,\,
   R_1^{(2)}(z) = 1\right\}\,,\\
\\
&& \displaystyle \left\{K_2 =  12C_F,\, R_2^{(1)}(x) = x,\,
   R_2^{(2)}(z) = z\right\}\,,\\
\\
\end{array}
\end{equation}
\begin{equation}
\begin{array}{rcl}
C_{gq}^{2,(1)} &:& \displaystyle {\rm LR}(z) = 2C_F\left[\frac{1+(1-z)^2}{z} \ln\left[
                                               z(1-z)\right]+z\right]\,,\\
  \\
&& \displaystyle {\rm SR}(z) = 2C_F\left[\frac{1+(1-z)^2}{z}\right]\,,\\
\\
&& \displaystyle \left\{K_1 =  4C_F,\, R_1^{(1)}(x) = 1 + 3x,\,
   R_1^{(2)}(z) = 1\right\}\,,\\
\\
&& \displaystyle \left\{K_2 =  -12C_F,\, R_2^{(1)}(x) = x,\,
   R_2^{(2)}(z) = z\right\}\,,\\
\\
&& \displaystyle \left\{K_3 =  -2C_F,\, R_3^{(1)}(x) = 1+x,\,
   R_3^{(2)}(z) = \frac1{z}\right\}\,,\\
\\
\end{array}
\end{equation}
\begin{equation}
\begin{array}{rcl}
C_{qg}^{2,(1)} &:&\displaystyle {\rm RL}(x) = \left[x^2+(1-x)^2\right]
                     \ln\left(\frac{1-x}{x}\right)
                     +2x(1-x)\,,\quad  {\rm RS}(x) = x^2+(1-x)^2\,,\\
\\
&& \displaystyle \left\{K_1 = 2,\, R_1^{(1)}(x) = - 1 + 6x-6x^2,\,
   R_1^{(2)}(z) = 1\right\}\,,\\
\\
&& \displaystyle \left\{K_2 =  1,\, R_2^{(1)}(x) = x^2+(1-x)^2,\,
   R_2^{(2)}(z) = \frac1{z}\right\}\,.
\end{array}
\end{equation}
Analogously, for the LO partonic cross sections we find that:
\begin{equation}
\begin{array}{rcl}
C_{qq}^{2,(0)} &:& \displaystyle {\rm LL} =1 \,.
\end{array}
\end{equation}
All the coefficients that are not mentioned are equal to zero. We can
now explicitly implement Eq.~(\ref{eq:MasterFormula}). The one thing
that is left to sort out is the structure of $F_2$ and $F_L$ in terms
of the appropriate PDF and FF combinations. Looking
Eq.~(\ref{eq:f1sidis}), we observe that none of the coefficient
functions depends on the particular quark flavour (this is a feature
of the ZM scheme). Therefore, simplifying the notation, we can rewrite
Eq.~(\ref{eq:f1sidis}) as:
\begin{equation}
\begin{array}{rcl}
  \displaystyle  F &=& \displaystyle C_{qq}  \sum_{q} e_q^2 \left[q  D_q +\overline{q}  D_{\overline{q}}\right]  + C_{gq}  \sum_{q} e_q^2
                       \left[q+\overline{q}\right]\,D_g  +  C_{qg}  g \sum_{q} e_q^2
                       \left [D_q +D_{\overline{q}}\right]\,,
\end{array}
\label{eq:structF2L}
\end {equation}
where now the sums run only over the quark flavours and not over the
antiflavours.

We know apply the same treatment to the polarised structure function
$g_1$ introduced in Eqs.~(35) and~(36) of
Ref.~\cite{deFlorian:1997zj}. The LO is trivial, Referring to
Eq.~(\ref{eq:DoubleFuncStruct}), one finds:
\begin{equation}
\begin{array}{rcl}
\Delta C_{qq}^{H,(0)} &:& \displaystyle {\rm LL} =1 \,,
\end{array}
\end{equation}
with all the rest being zero. The NLO hard cross sections relevant to
the inclusive production of an unpolarised hadron by mean of a
polarised hadronic projectile are given in Eqs.~(C.8)-(C.10). Their
explicit expressions read:
\begin{equation}
\begin{array}{rcl}
  \displaystyle \frac{\Delta C_{qq}^{H,(1)}}{2C_F} &=& \displaystyle -8\delta(1-x)\delta(1-z)+2\delta(1-x)
                                  \left(\frac{\ln
                                  (1-z)}{1-z}\right)_++ 
                                  \delta(1-x) \left[\frac{1+z^2}{1-z}\ln z+(1-z)-(1+z)\ln(1-z)\right] \\
  \\
                              &+&\displaystyle 
                                  2\left(\frac{\ln
                                  (1-x)}{1-x}\right)_+\delta(1-z)+2\left(\frac{1}{1-x}\right)_+\left(\frac{1}{1-z}\right)_+- \left(\frac{1}{1-x}\right)_+(1+z)\\
  \\
                              &+& \displaystyle \left[ 
                                  -\frac{1+x^2}{1-x}\ln x+(1-x)
                                  -(1+x)\ln(1-x)\right]\delta(1-z) -
                                  (1+x)\left(\frac{1}{1-z}\right)_++(2x+2z)\,,\\
  \\
  \displaystyle \frac{\Delta C_{gq}^{H,(1)}}{2C_F} &=& \displaystyle \delta (1-x) \left[\frac{1+(1-z)^2}{z} \ln\left[
                                               z(1-z)\right]+z\right]\\
  \\
                                            &+&\displaystyle \left(\frac{1}{1-x}\right)_+\frac{1+(1-z)^2}{z}\\
  \\
                              &+&  \displaystyle (1+x)\left(2-\frac{1}{z}\right) -2z\,,\\
  \\
  \Delta C_{qg}^{H,(1)} &=& \displaystyle \left[(x^2-(1-x)^2)
                     \ln\left(\frac{1-x}{x}\right)
                     +2x(1-x)\right]\delta (1-z) +(x^2-(1-x)^2)
                     \left(\frac{1}{1-z}\right)_+\\
\\
&+&\displaystyle (x^2-(1-x)^2) \left(\frac{1}{z} -2\right) \,.
\end{array}
\label{eq:cfg1}
\end{equation}
Based on this, the corresponding coefficients of
Eq.~(\ref{eq:DoubleFuncStruct}) are:
\begin{equation}
\begin{array}{rcl}
\Delta C_{qq}^{H,(1)} &:& \displaystyle {\rm LL} = -16 C_F\,,\quad {\rm LS} =
                        4C_F\,,\quad {\rm LR}(z) = 2C_F
                        \left[\frac{1+z^2}{1-z}\ln
                        z+(1-z)-(1+z)\ln(1-z)\right]\\
\\
&& \displaystyle {\rm SL} = 4C_F\,,\quad {\rm SS} = 4C_F\,,\quad {\rm
   SR}(z) = -2C_F(1+z)\\
\\
&& \displaystyle {\rm RL}(x) = 2C_F \left[ 
                                  -\frac{1+x^2}{1-x}\ln x+(1-x)
                                  -(1+x)\ln(1-x)\right]\,,\quad {\rm
   RS}(x) = -2C_F(1+x)\,,\\
\\
&& \displaystyle \left\{K_1 =  4C_F,\, R_1^{(1)}(x) = x,\,
   R_1^{(2)}(z) = 1\right\}\,,\\
\\
&& \displaystyle \left\{K_2 =  4C_F,\, R_2^{(1)}(x) = 1,\,
   R_2^{(2)}(z) = z\right\}\,,\\
\\
\end{array}
\end{equation}
\begin{equation}
\begin{array}{rcl}
\Delta C_{gq}^{H,(1)} &:& \displaystyle {\rm LR}(z) = 2C_F\left[\frac{1+(1-z)^2}{z} \ln\left[
                                               z(1-z)\right]+z\right]\,,\\
  \\
&& \displaystyle {\rm SR}(z) = 2C_F\left[\frac{1+(1-z)^2}{z}\right]\,,\\
\\
&& \displaystyle \left\{K_1 =  2C_F,\, R_1^{(1)}(x) = 1 + x,\,
   R_1^{(2)}(z) = 2-\frac1{z}\right\}\,,\\
\\
&& \displaystyle \left\{K_2 =  -4C_F,\, R_2^{(1)}(x) = 1,\,
   R_2^{(2)}(z) = z\right\}\,,
\end{array}
\end{equation}
\begin{equation}
\begin{array}{rcl}
\Delta C_{qg}^{H,(1)} &:&\displaystyle {\rm RL}(x) = \left[x^2-(1-x)^2\right]
                     \ln\left(\frac{1-x}{x}\right)
                     +2x(1-x)\,,\quad  {\rm RS}(x) = x^2-(1-x)^2\,,\\
\\
&& \displaystyle \left\{K_1 = 1,\, R_1^{(1)}(x) = x^2-(1-x)^2,\,
   R_1^{(2)}(z) = \frac1{z}-2\right\}\,.
\end{array}
\end{equation}

Unfortunately, moving to $\mathcal{O}(\alpha_s^2)$, \textit{i.e.}
NNLO, the factorised structure of the regular-regular term in
Eq.~(\ref{eq:DoubleFuncStruct}) valid for the NLO expressions does no
longer hold~\cite{Bonino:2024qbh}. As a consequence, we cannot write
the structure functions as a bi-linear combination of single
convolutions. It is therefore convenient to develop a code structure
able to deal with general double convolutions. In other words, the
goal is to create a class able to compute and store the object
$\Theta^{\beta\delta,\alpha\gamma}$ defined in
Eq.~(\ref{eq:DoubleZMconv}). To do so, we first need to identify the
general structure of the operator $O$ valid at any perturbative
order. This structure can always be brought into the following form:
\begin{equation}
  \begin{array}{lcccccc}
    \displaystyle O(y_1,y_2)&=&\displaystyle C_{\rm LL}\delta(1-y_1)
                                \delta(1-y_2)
    &+&\displaystyle \delta(1-y_1)\left[\sum_{m}C_{\rm
        LS}^{(m)}D_m(y_2)\right]&+&\displaystyle \delta(1-y_1)C_{\rm LR}(y_2)\\
\\
&+& \displaystyle \left[\sum_{n}C_{\rm
        LS}^{(n)}D_n(y_1)\right]\delta(1-y_2)&+&\displaystyle
                                                 \sum_{n,m}C_{\rm
                                                 SS}^{(n,m)}D_n(y_1)
                                                 D_m(y_2)
                                &+&\displaystyle \sum_{n}D_n(y_1)C_{\rm SR}^{(n)}(y_2)\\
\\
&+&\displaystyle C_{\rm RL}(y_1)\delta(1-y_2) &+&\displaystyle
                                                  \sum_{m}C_{\rm
                                                  RS}(y_1)D_m(y_2)&+&\displaystyle
                                                                      C_{\rm
                                                                      RR}(y_1,y_2)\,.
  \end{array}
  \label{eq:DoubleFuncStructGen}
\end{equation}
where at N$^p$LO the indices $n$ and $m$ run between zero and $2p-1$,
and:
\begin{equation}
D_i(y)=\left[\frac{\ln^i(1-y)}{1-y}\right]_+\,.
\end{equation}
In view of a numerical implementation, it is convenient to rewrite the
$+$-distribution in the following way:
\begin{equation}
D_i(y)=\left[\frac{\ln^i(1-y)}{1-y}\right]_+=\left[\frac{\ln^i(1-y)}{1-y}\right]_{x}+\frac{\ln^{i+1}(1-x)}{i+1}\delta(1-y)=\mathcal{D}_i(y)+\mathcal{K}_i(x)\delta(1-y)\,,
\end{equation}
where I have introduced the new distribution:
\begin{equation}
\mathcal{D}_i(y)=\left[\frac{\ln^i(1-y)}{1-y}\right]_{x}\,,
\end{equation}
that upon integration between zero and $x$ (rather than between zero
and one0 acts as standard $+$-distribution:
\begin{equation}
  \int_x^1 dy\, \mathcal{D}_i(y) f(y) = \int_x^1 dy
  \left[\frac{\ln^i(1-y)}{1-y}\right]_{x}f(y) = \int_x^1 dy\frac{\ln^i(1-y)}{1-y}\left[f(y)-f(1)\right]\,,
\end{equation}
and the residual term:
\begin{equation}
\mathcal{K}_i(x)=-\int_0^xdy\frac{\ln^i(1-y)}{1-y}=\frac{\ln^{i+1}(1-x)}{i+1}\,.
\end{equation}
With this definition at hand, the decomposition in
Eq.~(\ref{eq:DoubleFuncStructGen}) becomes:
\begin{equation}
\small
  \begin{array}{rcl}
    \displaystyle O(y_1,y_2)&=&\displaystyle \left[C_{\rm LL}+\sum_{n}C_{\rm LS}^{(n)}\mathcal{K}_n(x_1) +\sum_{m}C_{\rm LS}^{(m)}\mathcal{K}_m(x_2)+\sum_{n,m}C_{\rm SS}^{(m)}\mathcal{K}_n(x_1)\mathcal{K}_m(x_2)\right]\delta(1-y_1)
                                \delta(1-y_2)\\
\\
    &+&\displaystyle \delta(1-y_1)\left[\sum_{m}C_{\rm
        LS}^{(m)}\mathcal{D}_m(y_2)+\sum_{n,m}C_{\rm SS}^{(n,m)}\mathcal{K}_n(x_1)\mathcal{D}_m(y_2)\right]\\
\\
&+&\displaystyle \delta(1-y_1)\left[C_{\rm LR}(y_2)+\sum_{n}\mathcal{K}_{n}(x_1)C_{\rm SR}(y_2)\right]\\
\\
    &+&\displaystyle \left[\sum_{n}C_{\rm
        SL}^{(n)}\mathcal{D}_n(y_1)+\sum_{n,m}C_{\rm
        SS}^{(n,m)}\mathcal{D}_n(y_1)
        \mathcal{K}_m(x_2)\right]\delta(1-y_2)\\
\\
&+&\displaystyle \sum_{n,m} C_{\rm SS}^{(n,
    m)}\mathcal{D}_{n}(y_1)
    \mathcal{D}_{m}(y_2)\\
\\
&+&\displaystyle \sum_{n}\mathcal{D}_{n}(y_1)C_{\rm SR}^{(n)}(y_2)\\
   \\
&+&\displaystyle \left[C_{\rm RL}(y_1)+\sum_{m}C_{\rm RS}(y_1)\mathcal{K}_{m}(x_2)\right]\delta(1-y_2)\\
\\
&+&\displaystyle \sum_{m}C_{\rm RS}^{(n)}(y_1)\mathcal{D}_{m}(y_2)\\
   \\
                            &+&\displaystyle C_{\rm RR}(y_1,y_2)\,.
  \end{array}
\end{equation}
that can be generally written as:
\begin{equation}
\small
  \begin{array}{rcccccc}
    \displaystyle O(y_1,y_2)&=&\displaystyle \mathcal{C}_{\rm
                                LL}(x_1,x_2)\delta(1-y_1)\delta(1-y_2)&+&\displaystyle
                                                                          \delta(1-y_1)\mathcal{C}_{\rm
                                                                          LS}(x_1,y_2)&+&\displaystyle
                                                                                        \delta(1-y_1)\mathcal{C}_{\rm LR}(x_1,y_2)\\
\\
    &+&\displaystyle \mathcal{C}_{\rm
        SL}(y_1,x_2)\delta(1-y_2)&+&\displaystyle \mathcal{C}_{\rm
                                     SS}(y_1,y_2)&+&\displaystyle
                                                     \mathcal{C}_{\rm SR}(y_1,y_2)\\
   \\
&+&\displaystyle \mathcal{C}_{\rm
    RL}(y_1)\delta(1-y_2)&+&\displaystyle \mathcal{C}_{\rm RS}(y_1,y_2) &+&\displaystyle \mathcal{C}_{\rm RR}(y_1,y_2)\,.
  \end{array}
\end{equation}

The computation of a structure function is obtained by computing
integrals of the following form:
\begin{equation}
F(x_1,x_2) =
x_1 x_2\int_{x_1}^1\frac{dy_1}{y_1}\int_{x_2}^1\frac{dy_2}{y_2}
O(y_1,y_2) d^{(1)}\left(\frac{x_1}{y_1}\right)
  d^{(2)}\left(\frac{x_2}{y_2}\right)\,.
\end{equation}
Applying the usual interpolation procedure gives:
\begin{equation}
\begin{array}{rcl}
F(x_\beta,x_\delta) &=&\displaystyle
\int_{x_\beta}^1dy_1\int_{x_\delta}^1dy_2\,
O(y_1,y_2) \left[\frac{x_\beta}{y_1}d^{(1)}\left(\frac{x_\beta}{y_1}\right)\right]
  \left[\frac{x_\delta}{y_2}d^{(2)}\left(\frac{x_\delta}{y_2}\right)\right]\\
\\
&=&\displaystyle
    \sum_{\alpha=0}^{N_x}\sum_{\gamma=0}^{N_x} \overline{d}^{(1)}_\alpha
    \overline{d}^{(2)}_\gamma \left[\int_{x_\beta}^1 dy_1 \, w_{\alpha}^{(k)}\left(\frac{x_\beta}{y_1}\right) \int_{x_\delta}^1 dy_2\, w_{\gamma}^{(k)}\left(\frac{x_\delta}{y_2}\right)\,
    O(y_1,y_2)\right]\,.
\end{array}
\end{equation}
This allows us to defines a ``double'' operator:
\begin{equation}
\Theta^{\beta\alpha,\delta\gamma}=\int_{x_\beta}^1 dy_1 \, w_{\alpha}^{(k)}\left(\frac{x_\beta}{y_1}\right) \int_{x_\delta}^1 dy_2\, w_{\gamma}^{(k)}\left(\frac{x_\delta}{y_2}\right)\,
    O(y_1,y_2)\,,
\end{equation}
where the various $\mathcal{C}$ functions are involved as follows:
\begin{equation}
\small
  \begin{array}{rcl}
    \displaystyle \Theta^{\beta\alpha,\delta\gamma}&=&\displaystyle \delta_{\beta\alpha}\delta_{\delta\gamma}C_{\rm LL} \\
\\
&+&\displaystyle \delta_{\beta\alpha}\int_{x_\gamma}^1dy_2 \left\{w_\gamma^{(k)}\left(\frac{x_\delta}{y_2}\right)C_{\rm LS}(y_2)+w_\gamma^{(k)}\left(\frac{x_\delta}{y_2}\right) C_{\rm LR}(y_2)\right\}\\
\\
    &+&\displaystyle
        \int_{x_\alpha}^1dy_1\left\{w_\alpha^{(k)}\left(\frac{x_\beta}{y_1}\right)C_{\rm SL}(y_1) + w_\alpha^{(k)}\left(\frac{x_\beta}{y_1}\right)C_{\rm RL}(y_1) \right\}\delta_{\delta\gamma}\\
\\
&+&\displaystyle \int_{x_\alpha}^1dy_1 \int_{x_\gamma}^1dy_2
    \bigg\{w_\alpha^{(k)}\left(\frac{x_\beta}{y_1}\right)w_\gamma^{(k)}\left(\frac{x_\delta}{y_2}\right)C_{\rm
    SS}(y_1,y_2)+ w_\alpha^{(k)}\left(\frac{x_\beta}{y_1}\right)w_\gamma^{(k)}\left(\frac{x_\delta}{y_2}\right) C_{\rm SR}(y_1,y_2)\\
   \\
&+&\displaystyle 
    w_\alpha^{(k)}\left(\frac{x_\beta}{y_1}\right)w_\rho^{(k)}\left(\frac{x_\gamma}{y_2}\right)C_{\rm RS}(y_1,y_2)+
    w_\alpha^{(k)}\left(\frac{x_\beta}{y_1}\right) w_\gamma^{(k)}\left(\frac{x_\delta}{y_2}\right) C_{\rm RR}(y_1,y_2)\bigg\}\,.
  \end{array}
\end{equation}

Now 


\subsubsection{Drell Yan (DY)}

In this section we apply to the Drell-Yan (DY) process the same
procedure followed above for SIDIS. As a matter of fact, SIDIS and DY
are strictly connected in that DY can be regarded as the time-like
counterpart of SIDIS. As a consequence, the structure of the relevant
observables as well as the form of the expressions involved are very
similar. Therefore, the application of the method described above
should be straightforward. However, it appears that, even at one loop,
there is not a set of expressions that write the rapidity ($Y$) and
invariant-mass ($Q$) differential DY cross sections in terms of Mellin
convolutions. Refs.~\cite{Gehrmann:1997pi, Gehrmann:1997ez} present a
set of expressions that are in fact close to a form required for
implementation in APFEL++. However, due to a non-standard definition
of the $+$-prescription, these expressions are particularly cumbersome
to manipulate. As a consequence, we will start considering the
expressions given in Ref.~\cite{Bonvini:2010tp}, originally computed
in Ref.~\cite{Altarelli:1979ub}, for the unpolarised case that use a
different and convenient parameterisation of the convolution
integrals. Specifically, the cross section up to one loop reads:
\begin{equation}
  \begin{array}{rcl}
\displaystyle \frac{1}{\tau}\frac{d^2\sigma}{dQ^2 dY} &=&
\displaystyle \int_{\tau}^1\frac{dz}{z}\int_0^1 du\Big\{
                                                          L_{q\overline{q}}(x_1(z,u),x_2(z,u))\left[\delta(1-z)+a_s(Q)F(z,u)\right]\\
    \\
    &+&\displaystyle  L_{qg}(x_1(z,u),x_2(z,u)) a_s(Q)G(z,u)+
        L_{gq}(x_1(z,u),x_2(z,u)) a_s(Q)G(z,1-u)\Big\}\,,
  \end{array}
  \label{eq:DYalaAltarelli}
\end{equation}
where $\tau = Q^2/s$, being $\sqrt{s}$ the hadronic center of mass
energy and:
\begin{equation}
x_1(z,u) =
\frac{\xi_1}{\sqrt{z}}\sqrt{\frac{z+(1-z)u}{1-(1-z)u}}\,,\quad
x_2(z,u) =
\frac{\xi_2}{\sqrt{z}}\sqrt{\frac{1-(1-z)u}{z+(1-z)u}}\,,\quad\xi_{1,2}
= \sqrt{\tau}e^{\pm Y}\,.
\label{eq:variables}
\end{equation}
The partonic luminosities are defined as:
\begin{equation}
L_{ij}(x_1,x_2) = \sum_{ij} c_{ij}(Q) f_i^{(1)}(x_1,Q)
f_j^{(2)}(x_2,Q)\,,\quad i,j=g,q
\end{equation}
and where $c_{ij}$ are the appropriate couplings and $f$ the
PDFs. Finally, the perturbative functions $F$ and $G$ read:
\begin{equation}
  \begin{array}{rcl}
    \displaystyle \frac{F(z,u)}{2C_F} &=&\displaystyle
                             \left(\delta(u)+\delta(1-u)\right)\left[\delta(1-z)\left(\frac{\pi^2}{3}-4\right)+2(1+z^2)\left(\frac{\ln(1-z)}{1-z}\right)_+-\frac{1+z^2}{1-z}\ln
                                          z+1-z\right]\\
    \\
    &+&\displaystyle
        \frac{1+z^2}{1-z}\left[\left(\frac{1}{u}\right)_++\left(\frac{1}{1-u}\right)_+\right]-2(1-z)\,,\\
    \\
    \displaystyle \frac{G(z,u)}{2T_R}&=&\displaystyle \delta(u)\left[(z^2+(1-z)^2)\ln\frac{(1-z)^2}z+2z(1-z)\right]+(z^2+(1-z)^2)\left(\frac{1}{u}\right)_++2z(1-z)+(1-z)^2u\,.
  \end{array}
  \label{eq:NLOcorrectionsDY}
\end{equation}

The goal is now to express Eq.~(\ref{eq:DYalaAltarelli}) as a double
Mellin convolution over the variables $x_1$ and $x_2$, that is:
\begin{equation}
\displaystyle \frac{1}{\tau}\frac{d^2\sigma}{dQ^2 dY} = \int_{\xi_1}^1\frac{dx_1}{x_1}\int_{\xi_1}^1\frac{dx_2}{x_2}L_{ij}(x_1,x_2)C_{ij}\left(\frac{\xi_1}{x_1}, \frac{\xi_2}{x_2}\right)\,,
\end{equation}
identifying the functions $C_{ij}$.  To do so, we first invert the
variables in Eq.~(\ref{eq:variables}), finding:
\begin{equation}
  \begin{array}{rcl}
  z &=&\displaystyle  y_1 y_2\,,\\
  \\
    u &=&\displaystyle \frac{(1-y_1^2)y_2}{(1-y_1y_2)(y_1+y_2)}\,,
  \end{array}
\end{equation}
where we have defined $y_i=\xi_i/x_i$. The resulting Jacobian is such
that:
\begin{equation}
\int_{\tau}^1\frac{dz}{z}\int_0^1 du \dots =
\int_{\xi_1}^1\frac{dx_1}{x_1}\int_{\xi_2}^1\frac{dx_2}{x_2}\frac{2y_1
y_2(1+y_1 y_2)}{(1-y_1 y_2)(y_1+y_2)^2}\dots\,.
\end{equation}

The leading-order term does not actually require the change of
variable because it evaluates to:
\begin{equation}
  \begin{array}{rcl}
\displaystyle \left.\frac{1}{\tau}\frac{d^2\sigma}{dQ^2 dY}\right|_{\rm LO} &=&
\displaystyle \int_{\tau}^1\frac{dz}{z}\int_0^1 du
                                                          L_{q\overline{q}}(x_1(z,u),x_2(z,u))\delta(1-z)
                                                                                =
                                                                                \int_0^1
                                                                                du
                                                          L_{q\overline{q}}(x_1(1,u),x_2(1,u))\,.
  \end{array}
\end{equation}
Using the fact that $x_i(1,u)=\xi_i$, one immediately finds:
\begin{equation}
\displaystyle \left.\frac{1}{\tau}\frac{d^2\sigma}{dQ^2 dY}\right|_{\rm LO} =  L_{q\overline{q}}(\xi_1,\xi_2)=\int_{\xi_1}^1\frac{dx_1}{x_1}\int_{\xi_2}^1\frac{dx_2}{x_2}\delta\left(1-\frac{\xi_1}{x_1}\right) \delta\left(1-\frac{\xi_2}{x_2}\right) L_{q\overline{q}}(x_1,x_2)\,,
\end{equation}
so that:
\begin{equation}
  C_{q\overline{q}}^{(0)}(y_1,y_2) = \delta(1-y_1)\delta(1-y_2)\,. 
\end{equation}

Let us now move on to the next-to-leading order (NLO) corrections. We
start with the function $F$ in Eq.~(\ref{eq:NLOcorrectionsDY}) that is
the NLO correction to the $q\overline{q}$ channel. The term
proportional to $\delta(u)+\delta(1-u)$ is easy to address noticing
that:
\begin{equation}
  \begin{array}{l}
    \displaystyle x_1(z,0) = \xi_1\,,\quad x_2(z,0) = \frac{\xi_2}{z}\,,\\
    \\
    \displaystyle  x_1(z,1) = \frac{\xi_1}{z}\,,\quad x_2(z,0) = \xi_2\,,\\
  \end{array}
\end{equation}
or equivalently:
\begin{equation}
  \begin{array}{l}
    \displaystyle y_1(z,0) = 1\,,\quad y_2(z,0) = z\,,\\
    \\
    \displaystyle y_1(z,1) = z\,,\quad y_2(z,0) = 1\,.\\
  \end{array}
\end{equation}
This is such that:
\begin{equation}
  \begin{array}{l}
\displaystyle\int_{\tau}^1\frac{dz}{z}\int_0^1 du
    (\delta(u)+\delta(1-u)) T(z) =\\
    \\
    \displaystyle
\int_{\xi_1}^1\frac{dx_1}{x_1}\int_{\xi_2}^1\frac{dx_2}{x_2}\frac{2y_1
y_2(1+y_1 y_2)}{(1-y_1 y_2)(y_1+y_2)^2}\left[\frac
    {1-y_2^2}{2y_2}\delta(1-y_1)+\frac{1-y_1^2}{2y_1}\delta(1-y_2)\right]T(y_1 y_2)=\\
    \\
    \displaystyle
\int_{\xi_1}^1\frac{dx_1}{x_1}\int_{\xi_2}^1\frac{dx_2}{x_2}\left[\delta(1-y_1) T(y_2)+\delta(1-y_2) T(y_1)\right]
    \,.
  \end{array}
\end{equation}

In the second line of $F$ in Eq.~(\ref{eq:NLOcorrectionsDY}) there
is then the term:
\begin{equation}
-2\int_{\tau}^1\frac{dz}{z}\int_0^1 du (1-z) =
\int_{\xi_1}^1\frac{dx_1}{x_1}\int_{\xi_2}^1\frac{dx_2}{x_2}\frac{-4y_1
y_2(1+y_1 y_2)}{(y_1+y_2)^2}\,.
\end{equation}

% Finally, there is the term:
% \begin{equation}
%   \begin{array}{l}
% \displaystyle \int_{\tau}^1\frac{dz}{z}\int_0^1 du
%     \frac{1+z^2}{1-z}\left[\left(\frac{1}{u}\right)_++\left(\frac{1}{1-u}\right)_+\right]L_{q\overline{q}}(x_1(z,u), x_2(z,u))
%     =\\
%     \\
% \displaystyle \int_{\tau}^1\frac{dz}{z} \int_0^1 du \frac{1+z^2}{1-z}
%     \left[\frac{1-\delta(u)}{u}+\frac{1-\delta(1-u)}{1-u}\right]L_{q\overline{q}}(x_1(z,u), x_2(z,u)) = \\
%     \\
%     \displaystyle \int_{\tau}^1\frac{dz}{z} \int_0^1 du \frac{1+z^2}{1-z}
%     \left[\frac{1-\delta(u)-\delta(1-u)}{u(1-u)}\right]L_{q\overline{q}}(x_1(z,u), x_2(z,u)) = \\
%     \\
%     \displaystyle
%     \int_{\xi_1}^1\frac{dx_1}{x_1}\int_{\xi_2}^1\frac{dx_2}{x_2}\frac{2(1+y_1
%     y_2)(1+y_1^2y_2^2)}{(1+y_1)(1+y_2)}\left[\frac{1-\delta(u)-\delta(1-u)}{(1-y_1)(1-y_2)}\right]L_{q\overline{q}}(x_1,
%     x_2)=\\
%     \\
%     \displaystyle
%     \int_{\xi_1}^1\frac{dx_1}{x_1}\int_{\xi_2}^1\frac{dx_2}{x_2}\frac{2(1+y_1
%     y_2)(1+y_1^2y_2^2)}{(1+y_1)(1+y_2)}\left[\frac{1-\frac{1-y_2^2}{2y_2}\delta(1-y_1)-\frac{1-y_1^2}{2y_1}\delta(1-y_2)}{(1-y_1)(1-y_2)}\right]L_{q\overline{q}}(x_1,
%     x_2)=\\
%     \\
%     \displaystyle
%     \int_{\xi_1}^1\frac{dx_1}{x_1}\int_{\xi_2}^1\frac{dx_2}{x_2}\frac{1}{(1-y_1)(1-y_2)}\Bigg[\frac{2(1+y_1
%     y_2)(1+y_1^2y_2^2)}{(1+y_1)(1+y_2)}\\
%     \\
%     \displaystyle -\frac{1-y_2^4}{2y_2}\delta(1-y_1)-\frac{1-y_1^4}{2y_1}\delta(1-y_2)\Bigg]L_{q\overline{q}}(x_1,
%     x_2)
%     \end{array}
% \end{equation}


% Let us now consider the $G$ contribution in
% Eq.~(\ref{eq:NLOcorrectionsDY}). As $u\leftrightarrow 1-u$ corresponds
% to $y_1\leftrightarrow y_2$, it is sufficient to consider only, for
% example, the case with $G(z,u)$ as the term with $G(z,1-u)$ is
% obtained swapping $y_1$ and $y_2$. Considering the explicit expression
% in Eq.~(\ref{eq:NLOcorrectionsDY}), 


\newpage
\subsection{Advantage of a logarithmic grid}

Given the particular structure of the integral $I$ in
Eq.~(\ref{eq:ZMconv}), it turns out to be very convenient to use a
logarithmically distributed grid along with Lagrange interpolating
functions. Let us specifically consider the (massless) integrals:
\begin{equation}
I_{\beta\alpha} =
\int_{x_\beta}^1dy\,O(y)w_{\alpha}\left(\frac{x_\beta}{y}\right)\,.
\label{eq:integralex}
\end{equation}
A logarithmically-spaced grid is defined such that
$\ln x_{n+1}=\ln x_{n}+\delta x$ with $\delta x$ a positive
constant. In addition, we consider a set of Lagrange interpolating
functions of degree $\kappa$ polynomial in $\ln z$,
$\{w_{\alpha}(z)\}$, that thus have the following form (see section on
the interpolation):
\begin{equation}
  w_{\alpha}^{(k)}(z) = \sum_{j=0\atop j \leq
    \alpha}^{k}\theta(z-x_{\alpha-j})\theta(x_{\alpha-j+1}-z)\prod^{k}_{\delta=0\atop \delta\ne
    j}\left[\frac{\ln(z)-\ln(x_{\alpha-j+\delta})}{\ln(x_{\alpha})-\ln(x_{\alpha-j+\delta})}\right]\,.
\label{eq:LagrangeFormulaLog}
\end{equation}
Due to the fact that the grid is logarithmically distributed with step
$\delta x$, the function above can be rearranged as follows:
\begin{equation}
w_{\alpha}^{(k)}(z) = \sum_{j=0\atop j \leq \alpha}^{k}\theta(z-x_{\alpha-j})\theta(x_{\alpha-j+1}-z)\prod^{k}_{\delta=0\atop \delta\ne j}\left[\frac{1}{\delta x} \ln\left(\frac{z}{x_\alpha}\right)\frac{1}{j-\delta}+1\right].
\end{equation}
It is finally easy to see that Eq.~(\ref{eq:LagrangeFormulaLog}) is
such that:
\begin{equation}
w_{\alpha}(z) =
\widetilde{w}\left(\ln\frac{z}{x_\alpha}\right)\quad\Rightarrow\quad
w_{\alpha}\left(\frac{x_\beta}{y}\right) =
\widetilde{w}\left(\ln\frac{x_\beta}{x_\alpha}-\ln y\right)
=\widetilde{w}\left((\beta-\alpha)\delta x-\ln y\right)\,.
\end{equation}
Therefore, the integrand of the integral in Eq.~(\ref{eq:integralex})
only depends on the difference $\beta-\alpha$ and not on $\beta$ and
$\alpha$ separately. Since the lower bound is $x_\beta$, this symmetry
seems to broken at the level of the integral.  However, the symmetry
is preserved thanks to the support properties of the interpolating
functions $w_\alpha$ and the fact that the relevant functions (PDFs or
FFs) are zero at $x=1$. To see this, we consider the integration
limits in Eq.~(\ref{eq:intlims}) with $\eta=1$. They can be written
as:
\begin{equation}
  c =
  \mbox{max}(x_\beta,e^{(\beta-\alpha-1)\delta x}) \quad\mbox{and}\quad d =
  \mbox{min}(1,e^{(\beta-\alpha+\kappa)\delta x}) \,.
\label{eq:intlims1}
\end{equation}
While the limit $d$ is manifestly only dependent on the difference
$\beta-\alpha$, the limit $c$ is not. However, $c$ does not have this
symmetry only when $x_\beta$ is selected in place of
$e^{(\beta-\alpha-1)\delta x}$ and this can only happen when:
\begin{equation}
x_\beta > e^{(\beta-\alpha-1)\delta x}\,.
\label{eq:ineq}
\end{equation}
Since the last point of the grid is $x_{N_x}=1$, being $N_x$ the
number of grid intervals, one can write:
\begin{equation}
x_\beta = \frac{x_\beta}{x_{N_x}} = e^{(\beta-N_x)\delta x}.
\end{equation}
Finally, relying on the monotonicity of the exponential function, the
inequality in Eq.~(\ref{eq:ineq}) becomes:
\begin{equation}
  \beta-N_x > \beta-\alpha-1\quad\Leftrightarrow\quad \alpha > N_x-1\quad\Leftrightarrow\quad \alpha = N_x\,.
\end{equation}
Therefore, the integrals $I_{\beta N_x}$ do not respect the
``$\beta-\alpha$'' symmetry.  However, as mentioned above,
$I_{\beta N_x}$ will always multiply a function computed in
$x_{N_x}=1$. In all cases of interest this function is identically
zero at $x_{N_x}=1$ and thus the symmetry is effectively preserved. In
addition, $c$ in Eq.~(\ref{eq:intlims1}) is such that if
$\beta>\alpha$ one has $c\geq1$. But being $c$ the lower integration
bound of and since in Eq.~(\ref{eq:integralex}) the upper bound is 1,
one immediately has that $I_{\beta\alpha}=0$ for $\beta>\alpha$. The
consequence of these observations is that computing the integrals
$a_\alpha=I_{0\alpha}$ for $\alpha=0,\dots,N_x$ is enough to
reconstruct the full set of integrals $I_{\beta\alpha}$ because, in
matricial representation, $I$ will look like this:
\begin{equation}
\displaystyle I_{\beta\alpha} = 
\begin{pmatrix}
a_0 &  a_1 & a_2 & \cdots & a_{N_x} \\
 0  & a_0 & a_1 & \cdots & a_{N_x-1} \\
 0  & 0   &  a_0 & \cdots & a_{N_x-2} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
 0  &   0  &   0 & \cdots & a_0 
\end{pmatrix}\,.
\label{eq:MatrixRep}
\end{equation}
In conclusion, adopting a logarithmically-spaced grid allows one to
compute $N_x+1$ integrals rather than $(N_x+1)(N_x+2)/2$ integrals.

There is another aspect that matters in terms of numerical efficiency
of the computation of the integrals $I_{\beta\alpha}$. Given the
support region of the interpolating functions $w_\alpha$, the integral
in Eq.~(\ref{eq:integralex}) effectively reads:
\begin{equation}
I_{\beta\alpha} = \int_c^ddy\,O(y)w_{\alpha}\left(\frac{x_\beta}{y}\right)\,,
\end{equation}
with the integration limits given in Eq.~(\ref{eq:intlims}). These
limits can be rearranged as follows:
\begin{equation}
  c=\frac{x_\beta}{x_{{\rm min}[N_x,\alpha+1]}}\quad\mbox{and}\quad
  {d} = \frac{x_\beta}{x_{{\rm max}[\beta,\alpha-\kappa]}}\,,
\label{eq:intlimsind}
\end{equation}
which makes it manifest the index range covered by the integration
range. The basic observation is that the functions $w_\alpha$ are
piecewise in correspondence of the grid nodes. This feature makes a
numerical integration over the full range defined in
Eq.~(\ref{eq:intlimsind}) hard to converge due to the cusps at the
grid nodes. However, the functions $w_\alpha$ are smooth between two
consecutive nodes. Therefore, it turns out to be convenient to compute
the integrals $I_{\beta\alpha}$ by breaking the integration range as
follows:
\begin{equation}
I_{\beta\alpha} = \sum_{j={\rm max}[0, \alpha + 1 - N_x]}^{{\rm
    min}[\kappa, \alpha - \beta]}
\int_{x_\beta/x_{\alpha-j+1}}^{x_\beta/x_{\alpha-j}}
dy\,O(y)w_{\alpha}\left(\frac{x_\beta}{y}\right)\,,
\label{eq:finalformula}
\end{equation}
in such a way that the integrand of each single integral is a smooth
function and thus easier to integrate. Despite the number of
intergrals to be computed increases, this procedure makes the
computation faster and more accurate. Finally, if the grid is
logarithmically distributed, and one defines:
\begin{equation}
s = \exp\left[\delta x\right]\,,
\end{equation}
Eq.~(\ref{eq:finalformula}) can also be written as:
\begin{equation}
I_{\beta\alpha} = \sum_{j={\rm max}[0, \alpha + 1 - N_x]}^{{\rm
    min}[\kappa, \alpha - \beta]}  \int_{s^{\beta -\alpha + j-1}}^{s^{\beta -\alpha + j}} dy\,O(y)w_{\alpha}\left(\frac{x_\beta}{y}\right)\,.
\end{equation}
These are the basic objects computed by {\tt APFEL++} to define a
DGLAP-like operator. The distribution $O$ has the general structure:
\begin{equation}
O(y)=L\delta(1-y)+\sum_{n}S^{(n)}\left[\frac{\ln^n(1-y)}{1-y}\right]_++R(y)\,.
\end{equation}
Each of the three terms on the r.h.s. upon integration produce:
\begin{equation}
\small
\begin{array}{rcl}
\displaystyle \sum_j  \int_{s^{\beta -\alpha + j-1}}^{s^{\beta -\alpha + j}}
  dy\,\delta(1-y)w_{\alpha}\left(\frac{x_\beta}{y}\right)&=&\displaystyle\delta_{\alpha\beta}\\
\\
\displaystyle \sum_j  \int_{s^{\beta -\alpha +
  j-1}}^{s^{\beta -\alpha + j}} dy
  \left[\frac{\ln^n(1-y)}{1-y}\right]_+
  w_{\alpha}\left(\frac{x_\beta}{y}\right) &=&\displaystyle\sum_j  \int_{s^{\beta -\alpha + j-1}}^{s^{\beta -\alpha + j}} dy
  \frac{\ln^n(1-y)}{1-y}
  \left[w_{\alpha}\left(\frac{x_\beta}{y}\right)-\delta_{\alpha\beta}\right]\\
\\
&+&\displaystyle\frac{1}{n+1}\ln^{n+1}\left(1-\frac{1}{s}\right)\delta_{\alpha\beta}\\
\\
\displaystyle \sum_j  \int_{s^{\beta -\alpha + j-1}}^{s^{\beta -\alpha + j}} dy\,R(y) w_{\alpha}\left(\frac{x_\beta}{y}\right)\,,
\end{array}
\end{equation}
where the index $j$ is understood to run between
${\rm max}[0, \alpha + 1 - N_x]$ and
${\rm min}[\kappa, \alpha - \beta]$, and so that:
\begin{equation}
I_{\beta\alpha} =
\left[L+\overline{S}\left(\frac{1}{s}\right)\right]\delta_{\alpha\beta}+
\sum_j  \int_{s^{\beta -\alpha + j-1}}^{s^{\beta -\alpha + j}}
dy\left[S(y) \left(w_{\alpha}\left(\frac{x_\beta}{y}\right)-\delta_{\alpha\beta}\right)+R(y) w_{\alpha}\left(\frac{x_\beta}{y}\right)\right]\,,
\end{equation}
where we have defined:
\begin{equation}
S(y) = \sum_{n}S^{(n)} \frac{\ln^n(1-y)}{1-y}\,,\quad\mbox{and}\quad
\overline{S}\left(\frac{1}{s}\right)=\sum_{n}\frac{S^{(n)}}{n+1}\ln^{n+1}\left(1-\frac{1}{s}\right)\,.
\end{equation}
Notice that $S(y)$ is minus the derivative of $\overline{S}(y)$.

Similarly, we can work out the expression on a logarithmically
distributed grid of the double operator:
\begin{equation}
\Theta^{\beta\alpha,\delta\gamma}=\int_{x_\beta}^1 dy_1 \, w_{\alpha}^{(k)}\left(\frac{x_\beta}{y_1}\right) \int_{x_\delta}^1 dy_2\, w_{\gamma}^{(k)}\left(\frac{x_\delta}{y_2}\right)\,
    O(y_1,y_2)\,,
\end{equation}
where:
\begin{equation}
  \begin{array}{lcccccc}
     O(y_1,y_2)&=& C_{\rm LL}\delta(1-y_1)\delta(1-y_2) &+& \delta(1-y_1) C_{\rm LS}(y_2)&+& \delta(1-y_1)C_{\rm LR}(y_2)\\
\\
               &+&   C_{\rm SL}(y_1)\delta(1-y_2)&+& C_{\rm SS}(y_1,y_2) &+& C_{\rm SR}(y_1,y_2)\\
\\
               &+& C_{\rm RL}(y_1)\delta(1-y_2) &+& C_{\rm RS}(y_1,y_2)&+& C_{\rm RR}(y_1,y_2)\,,
  \end{array}
\end{equation}
with:
\begin{equation}
\begin{array}{l}
\displaystyle C_{\rm LS}(y_2)=\sum_{m}C_{\rm LS}^{(m)}\left[\frac{\ln^{m}(1-y_2)}{1-y_2}\right]_+\,,\\
\\
\displaystyle C_{\rm SL}(y_1)=\sum_{n}C_{\rm SL}^{(n)}\left[\frac{\ln^{n}(1-y_1)}{1-y_1}\right]_+\,,\\
\\
\displaystyle C_{\rm SS}(y_1,y_2)=\sum_{n,m}C_{\rm SS}^{(n,m)}\left[\frac{\ln^{n}(1-y_1)}{1-y_1}\right]_+\left[\frac{\ln^{m}(1-y_2)}{1-y_2}\right]_+\,,\\
\\
\displaystyle C_{\rm SR}(y_1,y_2)=\sum_{n}\left[\frac{\ln^{n}(1-y_1)}{1-y_1}\right]_+ C_{\rm SR}^{(n)}(y_2) \,,\\
\\
\displaystyle C_{\rm RS}(y_1,y_2)=\sum_{m}C_{\rm RS}^{(m)}(y_1)\left[\frac{\ln^{m}(1-y_2)}{1-y_2}\right]_+\,.
\end{array}
\end{equation}
The final result is:
\begin{equation}
\small
  \begin{array}{rcl}
    \Theta^{\beta\alpha,\delta\gamma}&=& \displaystyle\delta_{\alpha\beta}\left[C_{\rm LL}+C_{\rm L\overline{S}}\left(\frac{1}{s_2}\right)+C_{\rm\overline{S}L}\left(\frac{1}{s_1}\right)+C_{\rm\overline{S}\overline{S}}\left(\frac{1}{s_1},\frac{1}{s_2}\right)\right]\delta_{\gamma\delta}\\
\\
&+&\displaystyle 
\delta_{\alpha\beta}\sum_k  \int_{s_2^{\delta -\gamma + k-1}}^{s_2^{\delta -\gamma + k}}
dy_2\left[\left(C_{\rm LS}(y_2)+C_{\rm \overline{S}S}\left(\frac{1}{s_1},y_2\right)\right)\left(w_{\gamma}\left(\frac{x_\delta}{y_2}\right)-\delta_{\gamma\delta}\right)+\left(C_{\rm LR}(y_2) +C_{\rm \overline{S}R}\left(\frac{1}{s_1},y_2\right)\right) w_{\gamma}\left(\frac{x_\delta}{y_2}\right)\right]\\
\\
&+& \displaystyle 
\sum_j  \int_{s_1^{\beta -\alpha + j-1}}^{s_1^{\beta -\alpha + j}} dy_1\left[\left(w_{\alpha}\left(\frac{x_\beta}{y_1}\right)-\delta_{\alpha\beta}\right)
\left(C_{\rm SL}(y_1)+C_{\rm S\overline{S}}\left(y_1,\frac{1}{s_2}\right)\right)+\,w_{\alpha}\left(\frac{x_\beta}{y_1}\right) \left(C_{\rm RL}(y_1)+C_{\rm R\overline{S}}\left(y_1,\frac{1}{s_2}\right)\right)\right]\delta_{\gamma\delta}\\
\\
&+&\displaystyle 
\sum_{j,k}  \int_{s_1^{\beta -\alpha + j-1}}^{s_1^{\beta -\alpha + j}} dy_1 \int_{s_2^{\delta -\gamma + k-1}}^{s_2^{\delta -\gamma + k}}
dy_2\\
\\
&\times&\displaystyle \bigg[\left(w_{\alpha}\left(\frac{x_\beta}{y_1}\right)-\delta_{\alpha\beta}\right)C_{\rm SS}(y_1,y_2)\left(w_{\gamma}\left(\frac{x_\delta}{y_2}\right)-\delta_{\gamma\delta}\right)+\left(w_{\alpha}\left(\frac{x_\beta}{y_1}\right)-\delta_{\alpha\beta}\right)C_{\rm SR}(y_1,y_2)
    w_{\gamma}\left(\frac{x_\delta}{y_2}\right)\\
\\
&+&\displaystyle w_{\alpha}\left(\frac{x_\beta}{y_1}\right) C_{\rm RS} (y_1,y_2) \left(w_{\gamma}\left(\frac{x_\delta}{y_2}\right)-\delta_{\gamma\delta}\right)+w_{\alpha}\left(\frac{x_\beta}{y_1}\right) C_{\rm RR}(y_1,y_2) w_{\gamma}\left(\frac{x_\delta}{y_2}\right)\bigg]\,,
  \end{array}
\end{equation}
where
$j\in[{\rm max}[0, \alpha + 1 - N_{x_1}],{\rm min}[\kappa_1, \alpha -
\beta]]$
($k\in[{\rm max}[0, \gamma + 1 - N_{x_2}],{\rm min}[\kappa_2, \gamma -
\delta]]$), with $N_{x_1}$ ($N_{x_2}$) and $\kappa_1$ ($\kappa_2$) the
number of nodes and interpolation degree of the first (second) grid,
and $s_1$ ($s_2$) the logarithmic grid spacing of the first (second)
grid. Moreover, the following functions have been introduced:
\begin{equation}
\begin{array}{l}
\displaystyle C_{\rm L\overline{S}}\left(\frac{1}{s_2}\right)=\sum_{m}C_{\rm LS}^{(m)}\frac{1}{m+1}\ln^{m+1}\left(1-\frac{1}{s_2}\right)\,,\\
\\
\displaystyle C_{\rm \overline{S}L}\left(\frac{1}{s_1}\right)=\sum_{n}C_{\rm SL}^{(n)}\frac{1}{n+1}\ln^{n+1}\left(1-\frac{1}{s_1}\right)\,,\\
\\
\displaystyle C_{\rm \overline{S}S}\left(\frac{1}{s_1},y_2\right)=\sum_{n,m}C_{\rm SS}^{(n,m)}\left[\frac{1}{n+1}\ln^{n+1}\left(1-\frac{1}{s_1}\right)\right]\left[\frac{\ln^{m}(1-y_2)}{1-y_2}\right]_+\,,\\
\\
\displaystyle C_{\rm S\overline{S}}\left(y_1,\frac{1}{s_2}\right)=\sum_{n,m}C_{\rm SS}^{(n,m)}\left[\frac{\ln^{n}(1-y_1)}{1-y_1}\right]_+\left[\frac{1}{m+1}\ln^{m+1}\left(1-\frac{1}{s_2}\right)\right]\,,\\
\\
\displaystyle C_{\rm \overline{S}\overline{S}}\left(\frac{1}{s_1}, \frac{1}{s_2}\right)=\sum_{n,m}C_{\rm SS}^{(n,m)}\left[\frac{1}{n+1}\ln^{n+1}\left(1-\frac{1}{s_1}\right)\right]\left[\frac{1}{m+1}\ln^{m+1}\left(1-\frac{1}{s_2}\right)\right]\,,\\
\\
\displaystyle C_{\rm \overline{S}R}\left(\frac{1}{s_1},y_2\right) =\sum_{n}\frac{1}{n+1}\ln^{n+1}\left(1-\frac{1}{s_1}\right) C_{\rm SR}^{(n)}(y_2) \,,\\
\\
\displaystyle C_{\rm R\overline{S}}\left(y_1,\frac{1}{s_2}\right) =\sum_{m}C_{\rm RS}^{(m)}(y_1) \frac{1}{m+1}\ln^{m+1}\left(1-\frac{1}{s_2}\right)\,.
\end{array}
\end{equation}
From an implementation point of view, it is convenient to define:
\begin{equation}
\begin{array}{l}
\displaystyle \mathcal{C}_{\rm LL} (z_1,z_2) = C_{\rm LL}+C_{\rm L\overline{S}}\left(z_2\right)+C_{\rm\overline{S}L}\left(z_1\right)+C_{\rm\overline{S}\overline{S}}\left(z_1,z_2\right)\,,\\
\\
\displaystyle \mathcal{C}_{\rm LS} (z_1,z_2) = C_{\rm LS}(z_2)+C_{\rm \overline{S}S}\left(z_1,z_2\right)\,,\\
\\
\displaystyle \mathcal{C}_{\rm LR} (z_1,z_2) = C_{\rm LR}(z_2) +C_{\rm \overline{S}R}\left(z_1,z_2\right)\,,\\
\\
\displaystyle \mathcal{C}_{\rm SL} (z_1,z_2) = C_{\rm SL}(z_1)+C_{\rm S\overline{S}}\left(z_1,z_2\right)\,,\\
\\
\displaystyle \mathcal{C}_{\rm RL} (z_1,z_2) = C_{\rm RL}(z_1)+C_{\rm R\overline{S}}\left(z_1,z_2\right)\,,\\
\\
\displaystyle \mathcal{C}_{\rm SS} (z_1,z_2) = C_{\rm SS}(z_1,z_2)\,,\\
\\
\displaystyle \mathcal{C}_{\rm SR} (z_1,z_2) = C_{\rm SR}(z_1,z_2)\,,\\
\\
\displaystyle \mathcal{C}_{\rm RS} (z_1,z_2) = C_{\rm RS}(z_1,z_2)\,,\\
\\
\displaystyle \mathcal{C}_{\rm RR} (z_1,z_2) = C_{\rm RR}(z_1,z_2)\,,
\end{array}
\end{equation}
so that, one finally finds:
\begin{equation}
\small
  \begin{array}{rcl}
    \Theta^{\beta\alpha,\delta\gamma}&=& \displaystyle\delta_{\alpha\beta}\mathcal{C}_{\rm LL}\left(\frac{1}{s_1},\frac{1}{s_2}\right)\delta_{\gamma\delta}\\
    \\
                                     &+&\displaystyle 
                                         \delta_{\alpha\beta}\sum_k  \int_{s_2^{\delta -\gamma + k-1}}^{s_2^{\delta -\gamma + k}}
                                         dy_2\left[\mathcal{C}_{\rm LS}\left(\frac{1}{s_1},y_2\right)\left(w_{\gamma}\left(\frac{x_\delta}{y_2}\right)-\delta_{\gamma\delta}\right)+\mathcal{C}_{\rm LR}\left(\frac{1}{s_1},y_2\right) w_{\gamma}\left(\frac{x_\delta}{y_2}\right)\right]\\
    \\
                                     &+& \displaystyle 
                                         \sum_j  \int_{s_1^{\beta -\alpha + j-1}}^{s_1^{\beta -\alpha + j}} dy_1\left[\left(w_{\alpha}\left(\frac{x_\beta}{y_1}\right)-\delta_{\alpha\beta}\right)
                                         \mathcal{C}_{\rm SL}\left(y_1,\frac{1}{s_2}\right)+\,w_{\alpha}\left(\frac{x_\beta}{y_1}\right) \mathcal{C}_{\rm RL}\left(y_1,\frac{1}{s_2}\right)\right]\delta_{\gamma\delta}\\
    \\
                                     &+&\displaystyle 
                                         \sum_{j,k}  \int_{s_1^{\beta -\alpha + j-1}}^{s_1^{\beta -\alpha + j}} dy_1 \int_{s_2^{\delta -\gamma + k-1}}^{s_2^{\delta -\gamma + k}}
                                         dy_2\\
    \\
                                     &\times&\displaystyle \bigg[\left(w_{\alpha}\left(\frac{x_\beta}{y_1}\right)-\delta_{\alpha\beta}\right) \mathcal{C}_{\rm SS}(y_1,y_2)\left(w_{\gamma}\left(\frac{x_\delta}{y_2}\right)-\delta_{\gamma\delta}\right)+\left(w_{\alpha}\left(\frac{x_\beta}{y_1}\right)-\delta_{\alpha\beta}\right) \mathcal{C}_{\rm SR}(y_1,y_2)
                                              w_{\gamma}\left(\frac{x_\delta}{y_2}\right)\\
    \\
                                     &+&\displaystyle w_{\alpha}\left(\frac{x_\beta}{y_1}\right) \mathcal{C}_{\rm RS} (y_1,y_2) \left(w_{\gamma}\left(\frac{x_\delta}{y_2}\right)-\delta_{\gamma\delta}\right)+w_{\alpha}\left(\frac{x_\beta}{y_1}\right) \mathcal{C}_{\rm RR}(y_1,y_2) w_{\gamma}\left(\frac{x_\delta}{y_2}\right)\bigg]\,.
  \end{array}
\end{equation}

\section{GPD-related integrals}

When considering computations involving GPDs, another kind of integral
structure comes into play, that is:
\begin{equation}
J(x) = x\int_0^1\frac{dz}{z} O \left(\frac{x}{z},x\right) d(z)\,.
\label{eq:ZMconvERBL}
\end{equation}
This integral differs from that in Eq.~(\ref{eq:ZMconv}) in two
respects: the lower integration bound is zero rather than $x$ and the
operator $O$ may also depend explicitly on the external variable
$x$. These differences make the strategy devised for the numerical
computation of convolution integrals as in Eq.~(\ref{eq:ZMconv})
partially invalid. Let us discuss it in detail to motivate the
particular strategy adopted to compute Eq.~(\ref{eq:ZMconvERBL}) on a
grid.

As discussed above, logarithmically distributed grids are particularly
advantageous for integrals like those in Eq.~(\ref{eq:ZMconv}) because
they allow for a substantial reduction of the number of integrals to
be computed. However, logarithmic grids have two main
drawbacks. First, logarithmic grids that start from a relatively low
value of $x$ tend to be relatively sparse at large values of $x$. This
is a problem because all integrals that we are considering are such
that the function being interpolated is integrated up to $x=1$ and the
interpolation there can thus potentially degrade in accuracy. A
possible solution to this problem is to increase the density in a
stepwise fashion as $x$ gets closer to one. This produces locally
logarithmically distributed grids that allow one to exploit the
symmetry discussed above while making the grid denser, and thus more
accurate, at large $x$. The implementation of this procedure in
{\tt APFEL++} is achieved by means of \textit{locked} subgrids. In
practice, one starts with a logarithmic grid with a given lower bound,
\textit{e.g.}  $x_{\rm min}^{(0)}=10^{-5}$. Starting from a given
node, $x_{\rm min}^{(1)}$, the density of the grid is increased by
some integer factor. This procedure can be repeated an arbitrary
number of times as one moves towards large $x$, effectively defining
logarithmic subgrids that are increasingly denser and thus guarantee a
better interpolation accuracy. When dealing with integrals such as
that in Eq.~(\ref{eq:integralex}), the simplest way to exploit the
subgrid structure is to switch to a denser grid at the level of the
\textit{integral}, essentially using one grid when $x_\beta$ is below
the transition node and the other when it is above. The advantage of
this approach is that the integration procedure discussed above
applies verbatim with the only difference that, according to the
position of $x_\beta$, one grid is used rather than another. The
disadvantage of this procedure is that integrals with low values of
$x_\beta$ do not take advantage of the denser grids at large $x$. When
dealing with functions like PDFs that vanish rapidly as $x$ tends to
one, this is typically fine because the bulk of the integrals is
typically due to the region at $x\gtrsim x_\beta$. This usually makes
possible interpolation inaccuracies at large $x$
negligible. Unfortunately, when applied to integrals of the kind of
Eq.~(\ref{eq:ZMconvERBL}), this strategy may lead to severe
inaccuracies at large values of $x_\beta$. This is due to the fact
that the integral extends down to zero and this procedure does not
make use of the low-$x$ grids, effectively truncating the integral to
increasingly larger values of $x$ as $x_\beta$ approaches one.

The second problem is that logarithmic grids cannot get down to
$x = 0$. This is not an issue for the computation in
Eq.~(\ref{eq:ZMconv}) because the lower integration bound is $x$ and
thus, as long as the integral in not computed below the lower bound of
the grid, this does not introduce any inaccuracy. Conversely, this is
a potential problem for integrals like those in
Eq.~(\ref{eq:ZMconvERBL}) that extend down to zero for any value of
$x$. This may suggest that logarithmic grids are not suitable in the
first place for integrals as in Eq.~(\ref{eq:ZMconvERBL}). However, we
observe that GPD-related integrands are usually well-behaved around
$y=0$ implying that a logarithmic grid with lower bound $x_0$ close
enough to zero (\textit{e.g.}  $x_0=10^{-5}$) is expected to be enough
to make the contribution to the integral due to the region
$y\in[0,x_0]$ negligibly small. Therefore, also for the computation of
Eq.~(\ref{eq:ZMconvERBL}), we stick to logarithmically distributed
grids which allows us to use the same class for constructing and
managing grids developed to compute Eq.~(\ref{eq:ZMconv}).

In addition, GPD-related integrals, due to the explicit dependence of
the integrand on the external variable of the operator (see
Eq.~(\ref{eq:ZMconvERBL})), do not enjoy the translational
$\beta-\alpha$ symmetry discussed above. This is a major limitation in
that it does not allow us to reduce the number of integrals to be
computed.

These observation, along with the discussion on the interpolation
grid, leads us to take an approach in which the operator is computed
for all possible pairs $(\alpha,\beta)$ over a grid that extends as
low as possible in $x$ and is dense at large $x$: \textit{i.e.} the
joint grid.

Using the usual interpolation formula gives us that the integral in
Eq.~(\ref{eq:ZMconvERBL}) is computed on the grid as follows:
\begin{equation}
J(x_\beta) = \sum_{\alpha=0}^{N_x-1}J_{\beta\alpha}\overline{d}_\alpha\,.
\end{equation}
with $\overline{d}_\alpha = x_\alpha d(x_\alpha)$ and the assumption
$d(1)=0$, and with:
\begin{equation}
J_{\beta\alpha} =
\sum_{j=0}^{{\rm min}[\alpha,\kappa]}\int_{x_\beta/x_{\alpha-j+1}}^{x_\beta/x_{\alpha-j}} dy\,O(y,x_\beta)w_\alpha^{(\kappa)}\left(\frac{x_\beta}{y}\right)\,,
\end{equation}
where the indices $\alpha$ and $\beta$ are intended to run over the
joint grid. We point out again that in this case it is necessary to
compute all the $N_x^2$ integrals making up $J_{\beta\alpha}$.

\subsection{Principal-valued integrals}

Expressions for the operator $O$ in Eq.~(\ref{eq:ZMconvERBL}) often
involve singular terms that are to interpreted as Cauchy
principal-valued distributions. We are specifically concerned with
integrals of the following kind:
\begin{equation}
I={\rm PV}\int_x^{\infty}dy\,\frac{f(y)}{1-y}\,
\end{equation}
where $f(y)$ is a smooth function over the integration range. In order
to numerically treat this kind of integrals, in the document devoted
to GPDs the so-called $++$-distribution has been introduced such that:
\begin{equation}
I=\int_x^{\infty}dy\,\left(\frac{1}{1-y}\right)_{++}f(y)\,
\end{equation}
Similarly to the popular $+$-distribution, the $++$-distribution acts
upon integration in the following way:
\begin{equation}
\int_x^{\infty}dy\,\left(\frac{1}{1-y}\right)_{++}f(y) =
\int_x^{\infty}\frac{dy}{1-y}\left[f(y)-f(1)\left(1+\theta(y-1)\frac{1-y}{y}\right)\right]+f(1)\ln(1-x)
\,.
\label{eq:pluplusprescription}
\end{equation}
However, differently from the $+$-distribution, the $++$-distribution
does not regulate an otherwise divergent integral. It rather
rearranges the integrand in such a way that it is finite over the
integration range (the divergence at $y=1$ is no longer there) making
it numerically treatable. We now need to devise a way to compute this
integral within the interpolation framework. Indeed, the interpolation
procedure produces integrals as this:
\begin{equation}
I_{\beta\alpha}=\int_{x_{\beta}}^{\infty}dy\,\left(\frac{1}{1-y}\right)_{++}w_{\alpha}\left(\frac{x_{\beta}}{y}\right) \,,
\end{equation}
that need to be treated with care due to the support region of the
interpolation functions $w_\alpha$. In particular, we know that the
function $w_\alpha(x_\beta/y)$ has support
$y\in [x_\beta/x_{\alpha+1},x_\beta/x_{{\rm max}[0,\alpha-\kappa]}]$, so that:
\begin{equation}
\begin{array}{rcl}
I_{\beta\alpha}&=&\displaystyle
                   \int_{c}^{d}dy\,\left(\frac{1}{1-y}\right)_{++}w_{\alpha}\left(\frac{x_{\beta}}{y}\right)=\int_{c}^{\infty}dy\,\left(\frac{1}{1-y}\right)_{++}w_{\alpha}\left(\frac{x_{\beta}}{y}\right)\\
\\
&=&\displaystyle
    \int_c^{\infty}\frac{dy}{1-y}\left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-\delta_{\beta\alpha}\left(1+\theta(y-1)\frac{1-y}{y}\right)\right]+\delta_{\beta\alpha}\ln(1-c)\\
\\
&=&\displaystyle
    \int_c^{d}\frac{dy}{1-y}\left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-\delta_{\beta\alpha}\left(1+\theta(y-1)\frac{1-y}{y}\right)\right]+\delta_{\beta\alpha}\left[\ln(1-c)-\int_d^{\infty}\frac{dy}{1-y}\left(1+\theta(y-1)\frac{1-y}{y}\right)\right]\\
\\
&=&\displaystyle
    \int_c^{d}\frac{dy}{1-y}\left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-\delta_{\beta\alpha}\left(1+\theta(y-1)\frac{1-y}{y}\right)\right]+\delta_{\beta\alpha}\left[\ln(1-c)-\int_d^{\infty}dy\left(\frac{1}{1-y}+\frac{1}{y}\right)\right]\\
\\
&=&\displaystyle
    \int_c^{d}\frac{dy}{1-y}\left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-\delta_{\beta\alpha}\left(1+\theta(y-1)\frac{1-y}{y}\right)\right]+\delta_{\beta\alpha}\left[\ln(1-c)-\int^{1-d}_{-\infty}\frac{dz}{z}-\int_d^{\infty}\frac{dy}{y}\right] \\
\\
&=&\displaystyle
    \int_c^{d}\frac{dy}{1-y}\left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-\delta_{\beta\alpha}\left(1+\theta(y-1)\frac{1-y}{y}\right)\right]+\delta_{\beta\alpha}\left[\ln(1-c)+\int_{1-d}^{d}\frac{dy}{y}\right] \\
\\
&=&\displaystyle
    \int_c^{d}\frac{dy}{1-y}\left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-\delta_{\beta\alpha}\left(1+\theta(y-1)\frac{1-y}{y}\right)\right]+\delta_{\beta\alpha}\left[\ln(1-c)+\int_{d-1}^{d}\frac{dy}{y}\right] \\
\\
&=&\displaystyle
    \int_c^{d}\frac{dy}{1-y}\left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-\delta_{\beta\alpha}\left(1+\theta(y-1)\frac{1-y}{y}\right)\right]+\delta_{\beta\alpha}\left[\ln(1-c)-\ln\left(1-\frac{1}{d}\right)\right] \,,
\end{array}
\end{equation}
with:
\begin{equation}
c = \frac{x_\beta}{x_{\alpha+1}}\quad\mbox{and}\quad d = \frac{x_\beta}{x_{\alpha-\rm{min}[\alpha,\kappa]}}=\frac{x_\beta}{x_{\rm{max}[0,\alpha-\kappa]}}\,,
\end{equation}
and where we have used the fact that $d\geq 1$ for $\beta=\alpha$ and,
according to the Cauchy principal-value prescription:
\begin{equation}
\int_{-a}^{a}\frac{dy}{y} = 0\,,
\end{equation}
where $a$ could also be $\infty$.

\subsubsection{Principal-valued integrals in the DGLAP region}

When considering some specific GPD-related functions, such as the
matching functions for generalised transverse-momentum-dependent
(GTMD) distributions on GPDs, another kind of principal-valued
integrals appear. They look like this:
\begin{equation}
  J={\rm PV}\int_x^{1}dy\,\frac{f(y)}{1-\kappa y}\equiv
  \int_x^{1}dy\left(\frac{1}{1-\kappa y}\right)_{\rm PV}f(y)\,
\end{equation}
with $\kappa = \xi/x$ in the region where $x<\xi<1$, that corresponds
to $\kappa > 1$ and $\kappa < 1/x$, such that the singularity of the
integrand at $y = 1/\kappa$ is within the integration interval. As
done above, we can make this kind of integrals numerically amenable by
adding and subtracting the singularity:
\begin{equation}
  f\left(\frac1\kappa\right)\int_{1/\kappa}^{1}dy\,\frac{1}{1-\kappa y}\,.
\end{equation}
After some algebra, one finds:
\begin{equation}
  J = \int_x^1\frac {dy}{1-\kappa y}
  \left[f(y)-f\left(\frac{1}{\kappa}\right)\left(1+\theta\left(\kappa y-1\right)\frac{1-\kappa
        y}{\kappa
        y}\right)\right]+f\left(\frac{1}{\kappa}\right)\frac{1}{\kappa}\ln\left(\frac{\kappa(1-\kappa
      x)}{\kappa-1}\right)\,.
\label{eq:PVprescriptionxi}
\end{equation}

In order to carry out the usual interpolation procedure, we need to
compute integrals as this:
\begin{equation}
J_{\beta\alpha}=\int_{x_{\beta}}^{1}dy\,\left(\frac{1}{1-\kappa_\beta y}\right)_{\rm
PV}w_{\alpha}\left(\frac{x_{\beta}}{y}\right) \,.
\end{equation}
with $\kappa_\beta = \xi/x_\beta>1$. As above, the support region of
the interpolation functions $w_\alpha(x_\beta/y)$, that is
$y\in [x_\beta/x_{\alpha+1},x_\beta/x_{{\rm max}[0,\alpha-\kappa]}]\equiv[c,d]$,
requires some care. In particular, considering that the pole of the
integrand is at $y=1/\kappa_\beta$ and that $c<d$, if
$c>1/\kappa_\beta$ or if $d<1/\kappa_\beta$, one needs not use the
principal-value prescription because the pole is outside the
integration range. On the contrary, if $c<1/\kappa_\beta<d$, the pole
is within the integration range and thus the principal-value
prescription has to be used. In view of this analysis, we can
decompose the integrals $J_{\beta\alpha}$ in two components according
to whether the pole of the integrand falls into the integration range
or not:
\begin{equation}
\begin{array}{rcl}
J_{\beta\alpha}&=&\displaystyle \left[\theta(\xi-x_{\alpha+1})+\theta(x_{{\rm
  max}[0,\alpha-\kappa]}-\xi)\right]\int_{x_{\beta}}^{1}\frac{dy}{1-\kappa_\beta
  y}w_{\alpha}\left(\frac{x_{\beta}}{y}\right)\\
\\
&+&\displaystyle \theta(x_{\alpha+1}-\xi)\theta(\xi -x_{{\rm max}[0,\alpha-\kappa]}) \int_{x_{\beta}}^{1}dy\,\left(\frac{1}{1-\kappa_\beta y}\right)_{\rm
PV}w_{\alpha}\left(\frac{x_{\beta}}{y}\right)\,.
\end{array}
\end{equation}
Now, the first integral in the r.h.s. can be computed as is as the
integrand is regular, while the second integral requires the
principal-value prescription in Eq.~(\ref{eq:PVprescriptionxi}), yielding:
\begin{equation}
\small
\begin{array}{rcl}
  &&\displaystyle \int_{x_{\beta}}^{1}dy\,\left(\frac{1}{1-\kappa_\beta y}\right)_{\rm
                     PV}w_{\alpha}\left(\frac{x_{\beta}}{y}\right)\\
  \\
                 &=&\displaystyle \int_{x_\beta}^1\frac {dy}{1-\kappa_\beta y}
                     \left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-w_{\alpha}\left(\xi\right)\left(1+\theta\left(\kappa_\beta y-1\right)\frac{1-\kappa_\beta
                     y}{\kappa_\beta
                     y}\right)\right]+w_{\alpha}\left(\xi\right)\frac{1}{\kappa_\beta}\ln\left(\frac{\kappa_\beta(1-\kappa_\beta x_\beta)}{\kappa_\beta -1 }\right)\\
  \\
                 &=&\displaystyle \int_c^d\frac {dy}{1-\kappa_\beta y}
                     \left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-w_{\alpha}\left(\xi\right)\left(1+\theta\left(\kappa_\beta y-1\right)\frac{1-\kappa_\beta
                     y}{\kappa_\beta
                     y}\right)\right]\\
  \\
                 &+&\displaystyle
                     w_{\alpha}\left(\xi\right)\bigg[\frac{1}{\kappa_\beta}\ln\left(\frac{\kappa_\beta(1-\kappa_\beta
                     x_\beta)}{\kappa_\beta -1 }\right)-\int_{d}^1\frac {dy}{1-\kappa_\beta y}
                     \left(1+\theta\left(\kappa_\beta y-1\right)\frac{1-\kappa_\beta
                     y}{\kappa_\beta y}\right) \\
\\
  &-&\displaystyle \int_{x_\beta}^c\frac {dy}{1-\kappa_\beta y}
                     \left(1+\theta\left(\kappa_\beta y-1\right)\frac{1-\kappa_\beta
                     y}{\kappa_\beta y}\right)\bigg]\\
  \\
                 &=&\displaystyle \int_c^d\frac {dy}{1-\kappa_\beta y}
                     \left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-w_{\alpha}\left(\xi\right)\left(1+\theta\left(\kappa_\beta y-1\right)\frac{1-\kappa_\beta
                     y}{\kappa_\beta
                     y}\right)\right]\\
  \\
                 &+&\displaystyle
                     w_{\alpha}\left(\xi\right)
                     \frac{1}{\kappa_\beta}\bigg[\ln\left(\frac{\kappa_\beta(1-\kappa_\beta
                     x_\beta)}{\kappa_\beta -1
                     }\right)-\int_{\kappa_\beta d}^{\kappa_\beta}\frac {dz}{1-z}
                     \left(1+\theta\left(z-1\right)\frac{1-z}{z}\right) \\
\\
  &-&\displaystyle \int_{\kappa_\beta
      x_\beta}^{\kappa_\beta c}\frac {dz}{1-z}
                     \left(1+\theta\left(z-1\right)\frac{1-z}{z}\right)\bigg]\\
  \\
                 &=&\displaystyle \int_c^d\frac {dy}{1-\kappa_\beta y}
                     \left[w_{\alpha}\left(\frac{x_{\beta}}{y}\right)-w_{\alpha}\left(\xi\right)\left(1+\theta\left(\kappa_\beta y-1\right)\frac{1-\kappa_\beta
                     y}{\kappa_\beta
                     y}\right)\right]\\
  \\
                 &+&\displaystyle
                     w_{\alpha}\left(\xi\right)
                     \frac{1}{\kappa_\beta}\left[\ln\left(1-\kappa_\beta c\right)-\ln\left(1-k_\beta\frac{1}{k_\beta^2 d}\right)\right]\,.
\end{array}
\end{equation}

\newpage
\bibliographystyle{ieeetr}
\bibliography{bibliography}

\end{document}
